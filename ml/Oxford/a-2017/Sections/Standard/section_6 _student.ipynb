{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A\n",
    "\n",
    "## Standard Section 6: Logistic Regression and Bootstrapping\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Section Leaders: Albert Wu, Nathaniel Burbank<br/>**\n",
    "**Instructors: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>**Download this notebook from the CS109 repo or here:**</center>\n",
    "<center>**http://bit.ly/109_S6**</center>\n",
    "\n",
    "In this section we will be covering logistic regression using a cancer dataset. The goal is to get you familiarized with the implementation of logistic regresison and bootstrapping through the use of a dataset we haven't seen. The exercises below will help you be able to answer parts of Homework 5.\n",
    "\n",
    "Specifically, we will: \n",
    "    \n",
    "    1. Load in and get a feel for the prostate cancer dataset, which is similar in structure to our homework.\n",
    "    2. Fit a logistic regression model to only one predictor and understand how to interpret the results.\n",
    "    3. Compare to a linear regression model and then fit the logistic regression model to the entire dataset.\n",
    "    3. Take the results from the logistic regression model to create a bootstrap with confidence intervals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section we will be using the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 999)\n",
    "pd.set_option('display.max_columns', 999)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "from statsmodels.api import add_constant\n",
    "from statsmodels.regression.linear_model import RegressionResults\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Note --  Requires sklearn version .18 or higher  \n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from collections import Counter\n",
    "sns.set(style=\"ticks\")\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (13.0, 6.0)\n",
    "\n",
    "assert(sys.version_info.major==3),print(sys.version)\n",
    "# Python 3 or higher is required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup â€“ back to the titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "titanic = sns.load_dataset(\"titanic\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Subset to only cols we want\n",
    "titanic = titanic[['sex', 'age', 'class', 'survived']]\n",
    "\n",
    "# Rename 'class' col to 'pclass' to avoid namespace issues\n",
    "titanic.columns = ['sex', 'age', 'pclass', 'survived']\n",
    "\n",
    "#Drop any row with NA values in any col in dataframe\n",
    "titanic = titanic.dropna()\n",
    "\n",
    "is_female = pd.get_dummies(titanic.sex)['female']\n",
    "pclass_coded = pd.get_dummies(titanic.pclass)[['First','Second']]\n",
    "\n",
    "titanic_c = pd.concat([is_female,pclass_coded,titanic[['age','survived']]],axis=1)\n",
    "titanic_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train, test =  train_test_split(titanic_c, test_size=.2, random_state=123)\n",
    "model = sm.logit(formula=\"survived ~ female + First + Second + age\", data=train)\n",
    "model = model.fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all our predictors are significant here, and none of the confidence intervals include zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting logistic regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being a first class passenger increases the *logit of the probability* of surviving by **X**, relative to being a third-class passanger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The odds of First class passengers surviving are **X** times more likely than the odds of a third-class passenger surviving. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the probability of each passenger in our training set surviving, without using model.predict \n",
    "\n",
    "Hint:\n",
    "$$ p = \\cfrac{e^\\eta}{1+e^\\eta} =  \\cfrac{1}{1+e^{-\\eta}}  $$\n",
    "\n",
    "Where, $\\eta = \\alpha + \\beta_1 X_1 + ... + \\beta_n X_n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = titanic_c[['female','First','Second','age']]\n",
    "## Your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare with model.predict\n",
    "model.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (1): Load in our data and conduct basic EDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first load in our dataset below and look at the first few rows. Then, we use the describe function to get a sense of the data. **NOTE: A friendly reminder, this data is different from that of the homework. Please take care to make sure you do not accidentally use this data for the homework!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://github.com/albertw1/data/raw/master/section_6_cancer_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, `Y` here is the response variable (`Y` = 1 is cancer and `Y` = 0 is no cancer) and the 6033 predictor variables are gene expressions. The meaning of each column has no easily interpretable meaning. The way to think about it is that the bigger the value is for a gene, the more likely that gene could lead to someone having cancer. Each row represents a person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split this dataset up into a testing and training set using a 50-50 split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(9001)\n",
    "msk = np.random.rand(len(df)) < 0.5\n",
    "data_train_ns = df[msk] # ns for non-standarized data, we will standardize it right after this\n",
    "data_test_ns = df[~msk]\n",
    "data_train_ns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us standarize our data to be between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "data_train = # Your code goes here\n",
    "data_test = # Your code goes here\n",
    "\n",
    "data_train['Y']=data_train_ns['Y']\n",
    "data_test['Y']=data_test_ns['Y']\n",
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to let us subset and create convenient formats for parts in train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = #Your code goes here\n",
    "X_test  = #Your code goes here\n",
    "X_train_np = X_train.values # This is to create a numpy array version of the training and testing set.\n",
    "X_test_np  = X_test.values\n",
    "y_train = data_train['Y'].values\n",
    "y_test = data_test['Y'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (2): Using one single predictor to fit a Linear Regression Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use one single predictor variable, `Gene 10` and fit a linear regression model to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "X_train_gene_10 = sm.add_constant(X_train[\"Gene 10\"])\n",
    "X_test_gene_10 = sm.add_constant(X_test[\"Gene 10\"])\n",
    "\n",
    "ols = OLS(endog=y_train, exog=X_train_gene_10).fit()\n",
    "ols.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letâ€™s plot our modelâ€¦ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.regplot(x=X_train[\"Gene 10\"], y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hat_train = ols.predict(X_train_gene_10)\n",
    "y_hat_test = ols.predict(X_test_gene_10)\n",
    "print(y_hat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see anything \"wrong\" above with the predictions we made for the test set using the linear regression model? Hint: look at entry number 41."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice it is possible for us to convert the above values into a binary classification by the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hat_train_0_1 = y_hat_train >.5\n",
    "y_hat_test_0_1 = y_hat_test >.5\n",
    "\n",
    "print(\"Training Set Accuracy for Linear Regression: \", accuracy_score(y_train, y_hat_train_0_1))\n",
    "print(\"Test Set Accuracy for Linear Regression: \", accuracy_score(y_test, y_hat_test_0_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (3): Using one single predictor to fit a Logistic Regression Model instead\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can instead fit a logistic regression model onto our data and see how things change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logistic_mod = LogisticRegression(C=1000000, fit_intercept=True) #The C value here is a way to prevent regularization\n",
    "X_train_mul_wcons = sm.add_constant(X_train[\"Gene 10\"])\n",
    "X_test_mul_wcons = sm.add_constant(X_test[\"Gene 10\"])\n",
    "logistic_mod.fit(X_train_mul_wcons, y_train)\n",
    "\n",
    "y_hat_train_logistic_mod = logistic_mod.predict(X_train_mul_wcons)\n",
    "y_hat_test_logistic_mod = logistic_mod.predict(X_test_mul_wcons)\n",
    "\n",
    "print(\"Training Accuracy for Logistic Regression: \", accuracy_score(y_train, y_hat_train_logistic_mod))\n",
    "print(\"Test Accuracy for Logistic Regression: \", accuracy_score(y_test, y_hat_test_logistic_mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the training accuracy is basically the same, but the test accuracy with logsitic regression is better due to being able to better account for the cases where the response variable is very large and very small for `Gene 10`. The takeaway message here is that logistic regresison will outperform a model when your `Y` values tend to be very extreme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (4): Using all predictor variables in a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logistic_mod_all = LogisticRegression(C=100000, fit_intercept=True)\n",
    "X_train_mul_wcons = sm.add_constant(X_train)\n",
    "X_test_mul_wcons = sm.add_constant(X_test)\n",
    "\n",
    "logistic_mod_all.fit(X_train_mul_wcons, y_train)\n",
    "y_hat_train_logistic_mod_all = logistic_mod_all.predict(X_train_mul_wcons)\n",
    "y_hat_test_logistic_mod_all = logistic_mod_all.predict(X_test_mul_wcons)\n",
    "print(\"Training accuracy for Logistic Regression: \", accuracy_score(y_train, y_hat_train_logistic_mod_all))\n",
    "print(\"Test accuracy for Logistic Regression: \", accuracy_score(y_test, y_hat_test_logistic_mod_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = np.round(y_hat_test_logistic_mod_all)\n",
    "expected = y_test\n",
    "print(metrics.classification_report(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.crosstab(expected,predicted,margins=True, rownames=['Actual'], colnames=['Predicted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this matrix, we can see that of the people who actually **did not** have cancer, 6 were said to have cancer (false positive), while of the people who acutally **did** have cancer, 4 were mistakenly said to not have it (false negative). In biomedical applications, it usually is the case that false negatives tend to have greater consequences. For example, a patient would rather be told they mistakenly had cancer, which might lead to more testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (4): Incorporating Bootstrapping in a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea behind bootstrapping is to resample our data many times. For EACH time that we resample our data, we will fit the Logistic Regression model. Then for EACH resampling, we can find an associated confidence interval. Notice how we have many more predictors than observations. Intuitively, that creates a problem in that we do not have enough observations to \"represent\" each predictor varaible. Hence, the resampling gives us a way to approximate the case where we did indeed have enough observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "B = 100 # Number of iterations\n",
    "\n",
    "boot_coefs = np.zeros((X_train.shape[1]+1,B))\n",
    "\n",
    "for i in range(B):\n",
    "    #Your code goes here\n",
    "    boot_coefs[:,i] = logistic_mod_boot.coef_\n",
    "\n",
    "boot_coefs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to find the 95 percent confidence interval, we can use the `np.percentile` function to find the datapoint that corresponds to the upper and lower bounds. As an example, we have the upper datapoint for the 95% confidence interval. (Why is it 97.5?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ci_upper = np.percentile(boot_coefs, 97.5, axis=1)\n",
    "ci_lower = np.percentile(boot_coefs, 2.5, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ct significant predictors\n",
    "sig_b_ct = 0\n",
    "\n",
    "# if ci contains 0, then insignificant\n",
    "for i in range(len(ci_upper)):\n",
    "    if ci_upper[i]<0 or ci_lower[i]>0:\n",
    "        sig_b_ct += 1\n",
    "\n",
    "print(\"Significant coefficents at 5pct level = %i / %i\" % (sig_b_ct, X_train_np.shape[1]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
