{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A\n",
    "\n",
    "## Standard Section 7: Real World Classification Tradeoffs\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Section Leaders: Albert Wu, Nathaniel Burbank<br/>**\n",
    "**Instructors: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine** \n",
    "\n",
    "<center>**Download this notebook from the CS109 repo or here:**</center>\n",
    "<center>**http://bit.ly/109_S7**</center>\n",
    "\n",
    "In this section we will be covering real-world issues that arise with building classifiers. Specifically we will be discussing how to work with ROC curves, strategies to handle imbalanced datasets, and why the end metric that you’re actually interested in should be considered during model selection (i.e., you should think beyond just accuracy and classification statistics). \n",
    "\n",
    "Specifically, we will: \n",
    "    \n",
    "    1. Apply and interpret an ROC curve on a logistic classifier on the Titanic Dataset \n",
    "    2. Fit a logistic regression model to a (simulated) dataset of past customer purchases, and see if we can predict whether they’re pregnant \n",
    "    3. Build a classifier to help a non-profit target their fund-raising mailing more effectively, and see how different models perform remarkable different on an estimated net-revenue basis, despite having similar overall accuracy metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier as DecisionTree\n",
    "from sklearn.ensemble import RandomForestClassifier as RandomForest\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics, datasets\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review ROC Curves (Titanic Example) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start by load in the Titanic Dataset that we’ve become very familiar with over the last few sections, and fit a standard logistic regression classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "titanic = sns.load_dataset(\"titanic\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Subset to only cols we want\n",
    "titanic = titanic[['sex', 'age', 'class', 'survived']]\n",
    "\n",
    "# Rename 'class' col to 'pclass' to avoid namespace issues\n",
    "titanic.columns = ['sex', 'age', 'pclass', 'survived']\n",
    "\n",
    "#Drop any row with NA values in any col in dataframe\n",
    "titanic = titanic.dropna()\n",
    "\n",
    "is_female = pd.get_dummies(titanic.sex)['female']\n",
    "pclass_coded = pd.get_dummies(titanic.pclass)[['First','Second']]\n",
    "\n",
    "titanic_c = pd.concat([is_female,pclass_coded,titanic[['age','survived']]],axis=1)\n",
    "titanic_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C=1000000)\n",
    "train, test =  train_test_split(titanic_c, test_size=.2, random_state=123)\n",
    "X_train = train.iloc[:,:4]\n",
    "y_train = train.survived\n",
    "\n",
    "X_test = test.iloc[:,:4]\n",
    "y_test = test.survived\n",
    "\n",
    "logit.fit(X_train, y_train) \n",
    "print(logit.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Accuracy is about 75%. Let’s fit an ROC curve…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logit.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#making ROC curves for this model\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def make_roc(name, clf, ytest, xtest, ax=None, labe=5, proba=True, skip=0):\n",
    "    initial=False\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "        initial=True\n",
    "    if proba:#for stuff like logistic regression\n",
    "        fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n",
    "    else:#for stuff like SVM\n",
    "        fpr, tpr, thresholds=roc_curve(ytest, clf.decision_function(xtest))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    if skip:\n",
    "        l=fpr.shape[0]\n",
    "        ax.plot(fpr[0:l:skip], tpr[0:l:skip], '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    else:\n",
    "        ax.plot(fpr, tpr, '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    label_kwargs = {}\n",
    "    label_kwargs['bbox'] = dict(\n",
    "        boxstyle='round,pad=0.3', alpha=0.2,\n",
    "    )\n",
    "    if labe!=None:\n",
    "        for k in range(0, fpr.shape[0],labe):\n",
    "            #from https://gist.github.com/podshumok/c1d1c9394335d86255b8\n",
    "            threshold = str(np.round(thresholds[k], 2))\n",
    "            ax.annotate(threshold, (fpr[k], tpr[k]), **label_kwargs)\n",
    "    if initial:\n",
    "        ax.plot([0, 1], [0, 1], 'k--')\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title('ROC')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    return ax\n",
    "\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_roc(\"logistic\",logit, y_test,X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the blue line actually mean in this scenario? How might our model be different if we were at a different position on the line? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = logit.predict(test[['female','First','Second','age']])\n",
    "threshold = .5\n",
    "#predicted = threshold < pd.DataFrame(logit.predict_proba(test[['female','First','Second','age']]))[1]\n",
    "expected = test['survived']\n",
    "print(metrics.classification_report(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.crosstab(expected,predicted,margins=True,dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The row totals (87 died, 56 survived) above represent the *true* number of individuals in our test set that survived or perished on the titanic. The columns represent what our model predicted. Per the precision stats above, our model is getting about 3/4  of the test set correctly categorized, but still has a lot of false negatives and false positives.\n",
    "\n",
    "However, we can play with the discretization threshold for different outcomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threshold = .75\n",
    "predicted = threshold < pd.DataFrame(logit.predict_proba(test[['female','First','Second','age']]))[1]\n",
    "expected = test['survived']\n",
    "print(metrics.classification_report(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this dataset, there’s probably no real reason we’d want to optimize for anything but overall accuracy. (So we’d want our classifier to be in the upper left-hand corner of the ROC curve.) In the next example the situation is quite different… "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review oversampling (Pregnancy prediction example) \n",
    "\n",
    "This dataset was adapted from an example in *Data Smart* by John Foreman. It's a dataset of indicator variables about customer purchases in the past six-months for a (simulated) large retailer. Ala the scandal that Target [experienced](https://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-figured-out-a-teen-girl-was-pregnant-before-her-father-did/#3ceb310b6668) a few years back, the question is, can you predict customer’s pregnancy status based on their purchases alone? \n",
    "\n",
    "Separate training and test samples (1000 observations each) are provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "store_train = pd.read_csv(\"https://raw.githubusercontent.com/nathanielburbank/CS109/master/s7_data/store_data_train.csv\")\n",
    "store_test = pd.read_csv(\"https://raw.githubusercontent.com/nathanielburbank/CS109/master/s7_data/store_data_test.csv\")\n",
    "store_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "store_train.groupby(\"PREGNANT\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "store_test.groupby(\"PREGNANT\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that only ~4% of adult women are pregnant at any given time, so it’s a very imbalanced classification problem. However, we’ve significantly *oversampled* observations of pregnant women in our training data so as to ensure that our model does a good job of identifying true positives. What’s the downside of this approch? \n",
    "\n",
    "Anyway, let's build a logistic regression and evaluate the ROC curve..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C=1000000)\n",
    "X_train = store_train.iloc[:,:14]\n",
    "y_train = store_train.PREGNANT\n",
    "\n",
    "X_test = store_test.iloc[:,:14]\n",
    "y_test = store_test.PREGNANT\n",
    "\n",
    "logit.fit(X_train, y_train) \n",
    "print(logit.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_roc(\"logistic\",logit, y_test,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = logit.predict(X_test)\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.crosstab(y_test,predicted,margins=True,dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while our model does a great job of identifying true-positives (pregnant women) it comes at the cost of increased false-positives. Depending on what we were actually going to do this data, we might prefer one or another, and build a model that chose a different optimum on the ROC curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Outcome of a Fund-raising Campaign\n",
    "\n",
    "You are provided a data set containing details of mail sent to 95,412 potential donors for a fund-raising campaign of a not-for-profit organization. This data set also contains the amount donated by each donor. The task is to build a model that can identify which donors to mail in order to expected maximize net contribution, given the cost of mailing a flyer or package is \\$7 per donor. \n",
    "\n",
    "In this section, we will cast this problem as a classification problem: we build a model to classify each individual as a mail-worthy donor (will likely donate more than \\$7) or a un-mail-worthy donor (will likely donate less than \\$7). Again, our goal is to maximize the expected net contribution.\n",
    "\n",
    "The data is contained in the file `dataset.txt`. Each row contains 376 attributes for a donor, followed by the donation amount.\n",
    "\n",
    "## Step 1: Clean and explore the data\n",
    "\n",
    "First let's read and explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load and inspect the data\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/nathanielburbank/CS109/master/s7_data/dataset.txt', low_memory=False)  # low memory is set false for better type inference\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entries in the dataframe that read `'_'` are missing values. They should be replaced with NaN before you decide what to do with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Replace missing values with NaN\n",
    "data = data.replace('_', np.nan)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the NaN values appear only in a few columns. For the sake of expediency, we will simply drop the columns with missing values. **You might want to handle the missing data in a more sophisticated way**. (Such as on this week's HW...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove columns with missing values\n",
    "\n",
    "complete_cols = [column for column in data.columns if len(data[column][data[column].isnull()]) == 0]\n",
    "        \n",
    "data = data[complete_cols]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that some of the predictors are categorical and some are quantitative. We need to convert the real-valued predictor values into floating points and encode the categorical variables as integers (we will furthre convert the categorical variables into binary variable using one-hot-encoding). To decide which variables are quantitative and which are categorical, you need to interpret the meaning of each predictor and use your common sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Categoricals will be int or str (object), the rest float\n",
    "\n",
    "# List of columns to be converted to floating point\n",
    "to_float = ['HIT', 'MALEMILI', 'MALEVET', 'VIETVETS', 'WWIIVETS', 'LOCALGOV', 'STATEGOV', 'FEDGOV', 'NUMPRM12', \n",
    "           'CARDPM12', 'CARDPROM', 'NUMPROM', 'NGIFTALL', 'CARDGIFT']\n",
    "\n",
    "# Converted columns to floating point\n",
    "for feature_name in to_float:\n",
    "    data[feature_name] = data[feature_name].astype(float)\n",
    "\n",
    "# Columns between POP901 to AC2 should all be float\n",
    "index1 = data.columns.get_loc(\"POP901\")\n",
    "index2 = data.columns.get_loc(\"AC2\")\n",
    "\n",
    "for i in range(index1, index2 + 1):\n",
    "    data.iloc[:, i] = data.iloc[:, i].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Encode categorical variables\n",
    "def encode_categorical(array):\n",
    "    if not array.dtype == np.dtype('float64'):\n",
    "        return preprocessing.LabelEncoder().fit_transform(array) \n",
    "    else:\n",
    "        return array\n",
    "    \n",
    "# Categorical columns for use in one-hot encoder\n",
    "categorical = (data.dtypes.values != np.dtype('float64'))\n",
    "\n",
    "# Encode all labels\n",
    "data = data.apply(encode_categorical)\n",
    "\n",
    "# Get numpy array from data\n",
    "x = data.values[:, :-1]\n",
    "y = data.values[:, -1]\n",
    "\n",
    "# Apply one hot endcoing\n",
    "encoder = preprocessing.OneHotEncoder(categorical_features=categorical[:-1], sparse=False)  # Last value in mask is y\n",
    "x = encoder.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split our dataset into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train_val, y_test_val = train_test_split(x, y, test_size=0.6, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the response variable, amount donated, is a real-valued variable not a binary variable! We need to convert the amount donated into a binary value: 0 for under \\$7 and 1 for over \\$7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Threshold for class 0\n",
    "threshold = 7\n",
    "\n",
    "y_train = np.copy(y_train_val)\n",
    "y_test = np.copy(y_test_val)\n",
    "\n",
    "y_train[y_train_val > threshold] = 1\n",
    "y_train[y_train_val <= threshold] = 0\n",
    "\n",
    "y_test[y_test_val > threshold] = 1\n",
    "y_test[y_test_val <= threshold] = 0\n",
    "\n",
    "cost_per_donor = 7\n",
    "\n",
    "#Print some useful info for our test, train sets\n",
    "print('train data: ', x_train.shape)\n",
    "print('test data: ', x_test.shape)\n",
    "print('train class 0: {}, train class 1: {}'.format(len(y_train[y_train == 0]), len(y_train[y_train == 1])))\n",
    "print('test class 0: {}, test class 1: {}'.format(len(y_test[y_test == 0]), len(y_test[y_test == 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Establish the Baseline Models (for Sanity Check)\n",
    "\n",
    "What are the baseline models in this case? We can check off three basic models: \n",
    "\n",
    "1. a model that labels everything 1\n",
    "2. a model that labels everything 0\n",
    "3. a model that randomly guesses a label, 1 or 0\n",
    "\n",
    "Before implementing anything fancy, let's implement these baseline models and see how they do.\n",
    "\n",
    "**Note:** Again, think about accuracy in a **meaningful** way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function for computing the accuracy a given model on the entire test set, the accuracy on class 0 in the test set\n",
    "#and the accuracy on class 1\n",
    "score = lambda model, x_test, y_test: pd.Series([model.score(x_test, y_test), \n",
    "                                                 model.score(x_test[y_test==0], y_test[y_test==0]),\n",
    "                                                 model.score(x_test[y_test==1], y_test[y_test==1])],\n",
    "                                                index=['overall accuracy', 'accuracy on class 0', 'accuracy on class 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A model that labels everything 1\n",
    "class Pos_model(object):\n",
    "    def predict(self, x):\n",
    "        return np.array([1] * len(x))\n",
    "    def score(self, x, y):\n",
    "        y_pred = self.predict(x)\n",
    "        y_err = y - y_pred\n",
    "        return len(y_err[y_err == 0]) * 1. / len(y_err)\n",
    "    \n",
    "#A model that labels everything 0\n",
    "class Neg_model(object):\n",
    "    def predict(self, x):\n",
    "        return np.array([0] * len(x))\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        y_pred = self.predict(x)\n",
    "        y_err = y - y_pred\n",
    "        return len(y_err[y_err == 0]) * 1. / len(y_err)\n",
    "\n",
    "\n",
    "#A model that randomly labels things\n",
    "class Random_model(object):\n",
    "    def predict(self, x):\n",
    "        return np.random.randint(0, 2, len(x))\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        y_pred = self.predict(x)\n",
    "        y_err = y - y_pred\n",
    "        return len(y_err[y_err == 0]) * 1. / len(y_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_model = Pos_model()\n",
    "pos_model_scores = score(pos_model, x_test, y_test)\n",
    "\n",
    "neg_model = Neg_model()\n",
    "neg_model_scores = score(neg_model, x_test, y_test)\n",
    "\n",
    "random_model = Random_model()\n",
    "random_model_scores = score(random_model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Score Dataframe\n",
    "score_df = pd.DataFrame({'pos model': pos_model_scores,\n",
    "                         'neg model': neg_model_scores,\n",
    "                         'random model': random_model_scores})\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build Fancier Models\n",
    "\n",
    "Now that we have an idea of how baseline models perform, let's try to improve upon them with fancier classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Unweighted logistic regression\n",
    "unweighted_logistic = LogisticRegression()\n",
    "unweighted_logistic.fit(x_train, y_train)\n",
    "\n",
    "unweighted_log_scores = score(unweighted_logistic, x_test, y_test)\n",
    "print('unweighted log')\n",
    "\n",
    "\n",
    "#Weighted logistic regression\n",
    "weighted_logistic = LogisticRegression(class_weight='balanced')\n",
    "weighted_logistic.fit(x_train, y_train)\n",
    "\n",
    "weighted_log_scores = score(weighted_logistic, x_test, y_test)\n",
    "print('weighted log')\n",
    "\n",
    "#LDA\n",
    "lda = LDA()\n",
    "lda.fit(x_train, y_train)\n",
    "\n",
    "lda_scores = score(lda, x_test, y_test)\n",
    "print('lda')\n",
    "\n",
    "#QDA\n",
    "qda = QDA()\n",
    "qda.fit(x_train, y_train)\n",
    "\n",
    "qda_scores = score(qda, x_test, y_test)\n",
    "print('qda')\n",
    "\n",
    "#Decision Tree\n",
    "tree = DecisionTree(max_depth=6)\n",
    "tree.fit(x_train, y_train)\n",
    "\n",
    "tree_scores = score(tree, x_test, y_test)\n",
    "print('tree')\n",
    "\n",
    "#Random Forest\n",
    "rf = RandomForest()\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "rf_scores = score(rf, x_test, y_test)\n",
    "\n",
    "print('rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Score Dataframe\n",
    "score_df = pd.DataFrame({#'knn': knn_scores, \n",
    "                         'unweighted logistic': unweighted_log_scores,\n",
    "                         'weighted logistic': weighted_log_scores,\n",
    "                         'lda': lda_scores,\n",
    "                         'qda': qda_scores,\n",
    "                         'tree': tree_scores,\n",
    "                         'rf': rf_scores})\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So which model is better? Which out-performs our baseline models? What does \"better\" mean anyways? \n",
    "\n",
    "To perform meaningful model selection, we have to remember our task! We've been asked to create a classifier that will maximize expected net contribution! While accuracy is a helpful metric, it is not clear, by looking at these numbers, which model will generate a greater expected net contribution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Meaningful Model Evaluation\n",
    "\n",
    "To meaningully assess the effecitveness of our models, we have to evaluate them according to the utility function that computes the expected net contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------  expected_profit\n",
    "# A function that computes the expected net contribution generated from a mailing scheme based on\n",
    "# your classification of potential doners as mail-worthy or not mail-worthy\n",
    "# Input: \n",
    "#      y_true_val (the true amount donated by each individual)\n",
    "#      y_true (true class labels)\n",
    "#      y_pred (predicted class labels)\n",
    "# Returns: \n",
    "#      expected_profit (expected net contribution)\n",
    "\n",
    "def expected_profit(y_true_val, y_true, y_pred):\n",
    "    \n",
    "    profit = []\n",
    "    \n",
    "    for i in range(5000):\n",
    "    \n",
    "        sample = np.random.choice(len(y_true_val), len(y_true_val))\n",
    "\n",
    "        true_donations = y_true_val[sample]\n",
    "        true_labels = y_true[sample]\n",
    "        pred_labels = y_pred[sample]\n",
    "\n",
    "        pred_donors = pred_labels > 0\n",
    "\n",
    "        cost = (pred_donors).sum() * 7.\n",
    "\n",
    "        donations = true_donations[pred_donors].sum()\n",
    "\n",
    "        profit.append(donations - cost)\n",
    "        \n",
    "    expected_profit = np.mean(profit)\n",
    "    \n",
    "    return expected_profit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we implement a selection of classification models. You should try more (like KNN, SVM, logistic with polynomial decision boundaries etc.)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "profits = []\n",
    "\n",
    "#Unweighted logistic regression\n",
    "y_pred = unweighted_logistic.predict(x_test)\n",
    "\n",
    "profits.append(expected_profit(y_test_val, y_test, y_pred))\n",
    "\n",
    "print('unweighted log')\n",
    "\n",
    "#Weighted logistic regression\n",
    "y_pred = weighted_logistic.predict(x_test)\n",
    "\n",
    "profits.append(expected_profit(y_test_val, y_test, y_pred))\n",
    "\n",
    "print('weighted log')\n",
    "\n",
    "#LDA\n",
    "y_pred = lda.predict(x_test)\n",
    "\n",
    "profits.append(expected_profit(y_test_val, y_test, y_pred))\n",
    "\n",
    "print('lda')\n",
    "\n",
    "#QDA\n",
    "y_pred = qda.predict(x_test)\n",
    "\n",
    "profits.append(expected_profit(y_test_val, y_test, y_pred))\n",
    "\n",
    "print('qda')\n",
    "\n",
    "#Decision Tree\n",
    "y_pred = tree.predict(x_test)\n",
    "\n",
    "profits.append(expected_profit(y_test_val, y_test, y_pred))\n",
    "\n",
    "print('tree')\n",
    "\n",
    "#Random Forest\n",
    "y_pred = rf.predict(x_test)\n",
    "\n",
    "profits.append(expected_profit(y_test_val, y_test, y_pred))\n",
    "\n",
    "print('rf')\n",
    "\n",
    "#Positive Baseline Model\n",
    "y_pred = pos_model.predict(x_test)\n",
    "\n",
    "profits.append(expected_profit(y_test_val, y_test, y_pred))\n",
    "\n",
    "print('pos baseline')\n",
    "\n",
    "#Negative Baseline Model\n",
    "y_pred = neg_model.predict(x_test)\n",
    "\n",
    "profits.append(expected_profit(y_test_val, y_test, y_pred))\n",
    "\n",
    "print('neg baseline')\n",
    "\n",
    "#Random Baseline Model\n",
    "y_pred = random_model.predict(x_test)\n",
    "\n",
    "profits.append(expected_profit(y_test_val, y_test, y_pred))\n",
    "\n",
    "print('rand baseline')\n",
    "\n",
    "#Total possible profit (if all predictions are accurate)\n",
    "profits.append(expected_profit(y_test_val, y_test, y_test))\n",
    "\n",
    "print('total possible profit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "profits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Score Dataframe\n",
    "score_df = pd.DataFrame(data=np.array(profits).reshape((1, len(profits))), index=['net contribution'], columns=['unweighted log', 'weighted log', 'lda', 'qda', 'tree', 'rf', 'pos', 'neg', 'random', 'max possible profit'])\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did each model do - which is the best model? \n",
    "\n",
    "Compare the net contribution obtained from each model with the accuracy of each model. How is accuracy related to the net contribution? Is accuracy a good predictor of net contribution (meaning is accuracy a meaningful metric for evaluating our models)?\n",
    "\n",
    "What can we do to improve the performance of our models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
