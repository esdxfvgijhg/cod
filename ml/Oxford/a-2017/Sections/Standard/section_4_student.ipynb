{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A\n",
    "\n",
    "## Standard Section 4: Predictor types and feature selection\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Section Leaders: Nathaniel Burbank, Albert Wu<br/>**\n",
    "**Instructors: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>**Download this notebook from the CS109 repo or here:**</center>\n",
    "<center>**http://bit.ly/109_S4**</center>\n",
    "\n",
    "For this section, our goal is to continue discussing the complexities around different types of data features and thinking carefully about how different datatypes and collinearity issues can affect our models, whether our true goal is inference or prediction.\n",
    "\n",
    "Specifically, we will: \n",
    "    \n",
    "    1. Review different ways to plot multiple axes on a single figure in Matplotlib\n",
    "    2. Discuss different variable types, and techniques of “one-hot-encoding” our factor variables \n",
    "    3. Build a variable selection function that performs an exhaustive feature search overall all possible combinations of predictors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section we will be using the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/matplotlib/__init__.py:913: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 999)\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Note --  Requires sklearn version .18 or higher  \n",
    "from sklearn import metrics, datasets\n",
    "from collections import Counter\n",
    "import statsmodels.formula.api as sm\n",
    "from statsmodels.api import OLS\n",
    "from sklearn.linear_model import LinearRegression\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (13.0, 6.0)\n",
    "\n",
    "assert(sys.version_info.major==3),print(sys.version)\n",
    "# Python 3 or higher is required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (1): Review plotting on multiple axes within a single matplot lib figure\n",
    "\n",
    "![](https://i.imgur.com/XTzSuoR.png)\n",
    "source: http://matplotlib.org/faq/usage_faq.html\n",
    "\n",
    "Some of the code in the plots below in this section was also adapted from [this](http://matplotlib.org/faq/usage_faq.html) matplotlib tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot y1 and y2 on single figure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(0, 10, 0.2)\n",
    "y1 = np.sin(x)\n",
    "y2 = np.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Your code here \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot y1 and y2 on side by side axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Your code here \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot y1 and y2 on side by side axes, sharing the same y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Two subplots, unpack the axes array immediately\n",
    "\n",
    "## Your code here \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot y1 and y2 on separate (stacked) axes sharing the same x-axis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Two subplots, the axes array is 1-d\n",
    "\n",
    "## Your code here \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Four axes, returned as a 2-d array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Four axes, returned as a 2-d array\n",
    "f, axarr = plt.subplots(2, 2)\n",
    "axarr[0, 0].plot(x, y1)\n",
    "axarr[0, 0].set_title('Axis [0,0]')\n",
    "axarr[0, 1].scatter(x, y2)\n",
    "axarr[0, 1].set_title('Axis [0,1]')\n",
    "axarr[1, 0].plot(x, y1 ** 2)\n",
    "axarr[1, 0].set_title('Axis [1,0]')\n",
    "axarr[1, 1].scatter(x, y2 ** 2)\n",
    "axarr[1, 1].set_title('Axis [1,1]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (2): Logistic regression on Titanic data set \n",
    "\n",
    "In this part we’ll be using **[logistic regression](https://en.wikipedia.org/wiki/Logistic_regression)** to predict the likelihood of different passengers surviving the titanic disaster. We have not discussed logistic regression much in class, but for now it’s enough to know that logistic regression is typically preferable to linear regression when the outcome variable we’re predicting falls into discreet categories (in this case survived or didn’t survived) rather than a continuous range. \n",
    "\n",
    "First let's load the dataset... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic = sns.load_dataset(\"titanic\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Subset to only cols we want\n",
    "titanic = titanic[['sex', 'age', 'fare', 'class', 'survived']]\n",
    "\n",
    "# Rename 'class' col to 'pclass' to avoid namespace issues\n",
    "titanic.columns = ['sex', 'age', 'fare', 'pclass', 'survived']\n",
    "\n",
    "#Drop any row with NA values in any col in dataframe\n",
    "titanic = titanic.dropna()\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, let’s start ploting out data with box plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.boxplot(y=\"age\",x=\"survived\",hue=\"sex\",data=titanic)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age does not seem to be particularly informative. But there is something going on with gender?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=\"sex\", y=\"survived\", data=titanic)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we group by passenger class, we get an even more complete picture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=\"sex\", y=\"survived\", hue='pclass', data=titanic)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a series ‘is_female’, with 1s for women and 0s for men "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_female = ## Your code here \n",
    "is_female.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"One hot encode\" the passenger class column, and return a Dataframe with cols for 'First' and 'Second'\n",
    "\n",
    "Why don’t we want to keep all three columns? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pclass_coded = ## Your code here \n",
    "pclass_coded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a single dataframe with the encoded columns, plus 'age', 'fare' and 'survived' from the original dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic_c = ## Your code here \n",
    "titanic_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use train_test_split to divided our dataset in a 80% training set, and a 20% testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test =  train_test_split(titanic_c, test_size=.2, random_state=123)\n",
    "train.shape,test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a logistic regression model and print the summary statistics. Do all the predictors seem significant?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = sm.logit(formula=\"survived ~ female + First + Second + age + fare\", data=train)\n",
    "model = model.fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm…it seems like the fare predictor is not really helpful in our model. What’s going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = sns.boxplot(titanic.fare,titanic.pclass)\n",
    "ax.set_xlim([0,250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, fare clearly highly correlated with passenger class. Let’s drop it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = sm.logit(formula=\"survived ~ female + First + Second + age\", data=train)\n",
    "model = model.fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let’s test our model on the set-aside test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = np.round(model.predict(test[['female','First','Second','age']]))\n",
    "expected = test['survived']\n",
    "print(metrics.classification_report(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.crosstab(expected,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The row totals (87 died, 56 survived) above represent the true number of individuals in our test set that survived or perished on the titanic. The columns represent what our model predicted. Per the precision stats above, our model is getting about 3/4  of the test set correctly categorized, but still has a lot of false negatives and false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (3) Model Selection via exhaustive search selection\n",
    "\n",
    "The data set for this problem contains 10 simulated predictors and a response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/nathanielburbank/CS109/master/data/sec4_dataset.txt')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visually inspecting the data set, do we find that some of the predictors are correlated amongst themselves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the cofficient of correlation between each pair of predictors, and visualize the matrix of correlation coefficients using a heat map. Do the predictors fall naturally into groups based on the correlation values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were asked to select a minimal subset of predictors based on the correlation information in order to build a good regression model, how many predictors will you pick, and which ones will you choose? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Model Selection Criterion\n",
    "\n",
    "\n",
    "### Bayesian Information Criterion (BIC)\n",
    "\n",
    "– Generally BIC = -2 x Log-likehood + 2 x log(K)\n",
    "\n",
    "– For least-squares regression specifically: \n",
    "\n",
    "$$BIC = n \\log \\Big(\\frac{RSS}{n}\\Big) + \\log(n)*K$$\n",
    "\n",
    "Where:\n",
    "RSS = Residual Sum of Squares\n",
    "\n",
    "n = the number of obervations \n",
    "\n",
    "K = the number of features in our model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (b): Selecting minimal subset of predictors\n",
    "\n",
    "- Apply the Exhaustive search variable selection methods discussed in class to choose a minimal subset of predictors that yield high prediction accuracy. Use the Bayesian Information Criterion (BIC) to choose the subset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def exhaustive_search_selection(x, y):\n",
    "    \"\"\"Exhaustively search predictor combinations. .\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : DataFrame of predictors/features\n",
    "    y : response varible \n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    \n",
    "    Dataframe of model comparisons and OLS Model with \n",
    "    lowest BIC for subset with highest R^2\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # total no. of predictors\n",
    "    d = x.shape[1]\n",
    "    predictors = x.columns\n",
    "    overall_min_bic = 10000 # A big number \n",
    "    output = dict()\n",
    "    \n",
    "    # Outer loop: iterate over sizes 1 .... d\n",
    "    for k in range(1,d):\n",
    "        \n",
    "        max_r_squared = -10000 # A small number\n",
    "        \n",
    "        # Enumerate subsets of size ‘k’\n",
    "        subsets_k = itertools.combinations(predictors, k)\n",
    "        \n",
    "        # Inner loop: iterate through subsets_k\n",
    "        for subset in subsets_k:\n",
    "            # Fit regression model using ‘subset’ and calculate R^2 \n",
    "            # Keep track of subset with highest R^2\n",
    "            \n",
    "            features = list(subset)\n",
    "            x_subset = x[features]\n",
    "            \n",
    "            model = OLS(y, x_subset)\n",
    "            results = model.fit()\n",
    "            r_squared = results.rsquared\n",
    "            \n",
    "            # Check if we get a higher R^2 value than than current max R^2, \n",
    "            # if so, update our best subset \n",
    "            if(r_squared > max_r_squared):\n",
    "                max_r_squared = r_squared\n",
    "                best_subset = features\n",
    "                best_model = model\n",
    "                best_formula = \"y ~ {}\".format(' + '.join(features))\n",
    "        \n",
    "        results = best_model.fit()\n",
    "        bic = results.bic\n",
    "        if bic < overall_min_bic:\n",
    "            overall_min_bic = bic \n",
    "            best_overall_subset = best_subset\n",
    "            best_overall_rsquared = results.rsquared\n",
    "            best_overall_formula = best_formula\n",
    "            best_overall_model = best_model\n",
    "        \n",
    "        #print(\"For k={0} the best model is {1} with bic={2:.2f} and R^2={3:.4f}\".format(k,best_formula,bic,results.rsquared))\n",
    "        output[k] = {'best_model':best_formula, 'bic':bic,'r_squared':results.rsquared}\n",
    "        \n",
    "    print(\"The best overall model is {0} with bic={1:.2f} and R^2={2:.3f}\".format(best_overall_formula,overall_min_bic, best_overall_rsquared))\n",
    "    \n",
    "    return pd.DataFrame(output).T,best_overall_model     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = data.iloc[:,:-1]\n",
    "y = data.iloc[:,-1:]\n",
    "stats,model = exhaustive_search_selection(x,y)\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the chosen subsets match the ones you picked using the correlation matrix you had visualized in Part (a)?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
