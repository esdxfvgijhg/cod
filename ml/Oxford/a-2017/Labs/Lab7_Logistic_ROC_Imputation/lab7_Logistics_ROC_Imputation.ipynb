{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide": true
   },
   "source": [
    "# Lab 7 - Logistic Regression, ROCs and imputation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline\n",
    "import seaborn.apionly as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asymmetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'd like to simply notice that our data set is very highly asymmetric, with positives, or people who churned, only making up 14-15% of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gssdata=pd.read_csv(\"gssdata4.csv\")\n",
    "gssdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poorhealth = np.where(gssdata['health'] == 'poor',1,0)\n",
    "excellenthealth = np.where(gssdata['health'] == 'excellent',1,0)\n",
    "fairhealth = np.where(gssdata['health'] == 'fair',1,0)\n",
    "gssdata['poorhealth'] = poorhealth\n",
    "gssdata['fairhealth'] = fairhealth\n",
    "gssdata['excellenthealth'] = excellenthealth\n",
    "100*gssdata.poorhealth.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This means that a classifier which predicts EVERY respondent to not be in poor health will have an accuracy rate of 93-94%. \n",
    "\n",
    "But is accuracy the correct metric?  What may be a better measure of accuracy here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a logistic model ignoring missingness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by fitting a logistic regression model to predict poor health based on several of the other predictors in the model.  [in your HW, you will be asked to regularize (with Cross Validation) to make sure not to overfit to the data, but we will simploify things here.\n",
    "\n",
    "First we need to do a small amount of data clean-up (ignoring missingness for now in income).  Best practiuce would be to split into train and test first before looking at the data, but again, we're simplifying it for illustrative purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating dummies two ways\n",
    "gssdata['female'] = 1*(gssdata['sex'] ==  'female')\n",
    "dummy_vars = pd.get_dummies(gssdata[['sexornt','partyid','race']])\n",
    "gssdata = gssdata.join(dummy_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's get a sense of the data we have\n",
    "print(gssdata.shape)\n",
    "gssdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missingness approach \\#1: remove observations. \n",
    "We do not know how sklearn will treat the missing values (the \\texttt{NaN}s), so we should do handle them ourselves.  As a base case, let's remove all observations with missingness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: don't be fooled by the name...this is the data set the only includes the 'full' aka 'complete' observations\n",
    "gssdata_full = gssdata.dropna(how = 'any')\n",
    "print(gssdata_full.shape)\n",
    "\n",
    "# a quick check to see how dropping observations affected the amount of poor health individuals\n",
    "print(100*gssdata_full.poorhealth.mean())\n",
    "gssdata_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will split the data before fitting any models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "itrain, itest = train_test_split(range(gssdata_full.shape[0]), train_size=0.50)\n",
    "#gsstemp = gssdata_full.drop(['health','fairhealth','goodhealth','excellenthealth','sex','sexornt','partyid','race'],axis=1)\n",
    "gsstemp = gssdata_full[['age','educ','female','partyid_dem','partyid_rep','income']]\n",
    "X_train = gsstemp.iloc[itrain, :]\n",
    "X_test = gsstemp.iloc[itest, :]\n",
    "y_train = gssdata_full['poorhealth'].iloc[itrain]\n",
    "y_test = gssdata_full['poorhealth'].iloc[itest]\n",
    "\n",
    "y_train.shape, X_train.shape, y_test.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Your code here: fit a logistic model with C=1000000 and evaluate classification accuracy on the test set.\n",
    "#####################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember the Confusion matrix? We reproduce it here for convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the samples that are +ive and the classifier predicts as +ive are called True Positives (TP)\n",
    "- the samples that are -ive and the classifier predicts (wrongly) as +ive are called False Positives (FP)\n",
    "- the samples that are -ive and the classifier predicts as -ive are called True Negatives (TN)\n",
    "- the samples that are +ive and the classifier predicts as -ive are called False Negatives (FN)\n",
    "\n",
    "A classifier produces a confusion matrix which looks like this:\n",
    "\n",
    "![confusionmatrix](./confusionmatrix_360.png)\n",
    "\n",
    "\n",
    "IMPORTANT NOTE: In sklearn, to obtain the confusion matrix in the form above, always have the observed `y` first, i.e.: use as `confusion_matrix(y_true, y_pred)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test,logit.predict(X_test)))\n",
    "yhats = logit.predict_proba(X_train)\n",
    "hist = plt.hist(yhats[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#manually making confusion table from a different threshold\n",
    "def t_repredict(est, t, xtest):\n",
    "    probs = est.predict_proba(xtest)\n",
    "    p0 = probs[:,0]\n",
    "    p1 = probs[:,1]\n",
    "    ypred = (p1 > t)*1\n",
    "    return ypred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# your code here: look at other thresholds for confusion matrix\n",
    "#####################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making ROC curves for this model\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def make_roc(name, clf, ytest, xtest, ax=None, labe=5, proba=True, skip=0):\n",
    "    initial=False\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "        initial=True\n",
    "    if proba:#for stuff like logistic regression\n",
    "        fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n",
    "    else:#for stuff like SVM\n",
    "        fpr, tpr, thresholds=roc_curve(ytest, clf.decision_function(xtest))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    if skip:\n",
    "        l=fpr.shape[0]\n",
    "        ax.plot(fpr[0:l:skip], tpr[0:l:skip], '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    else:\n",
    "        ax.plot(fpr, tpr, '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    label_kwargs = {}\n",
    "    label_kwargs['bbox'] = dict(\n",
    "        boxstyle='round,pad=0.3', alpha=0.2,\n",
    "    )\n",
    "    if labe!=None:\n",
    "        for k in range(0, fpr.shape[0],labe):\n",
    "            #from https://gist.github.com/podshumok/c1d1c9394335d86255b8\n",
    "            threshold = str(np.round(thresholds[k], 2))\n",
    "            ax.annotate(threshold, (fpr[k], tpr[k]), **label_kwargs)\n",
    "    if initial:\n",
    "        ax.plot([0, 1], [0, 1], 'k--')\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title('ROC')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    return ax\n",
    "\n",
    "sns.set_context(\"poster\")\n",
    "ax=make_roc(\"logistic\",logit, y_test, X_test, labe=10, skip=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get back the data with missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first build a model to impute using data without missing \n",
    "hist = plt.hist(gssdata_full['income'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missingness approach \\#2: impute the mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#back to the original data set with missingness, make a copy, and then impute the mean\n",
    "gssdata2 = gssdata.copy()\n",
    "gssdata2['income'] = gssdata['income'].fillna(gssdata_full['income'].mean())\n",
    "hit = plt.hist(gssdata2['income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "itrain, itest = train_test_split(range(gssdata2.shape[0]), train_size=0.50)\n",
    "#gsstemp = gssdata_full.drop(['health','fairhealth','goodhealth','excellenthealth','sex','sexornt','partyid','race'],axis=1)\n",
    "gsstemp = gssdata2[['age','educ','female','partyid_dem','partyid_rep','income']]\n",
    "X_train2 = gsstemp.iloc[itrain, :]\n",
    "X_test2 = gsstemp.iloc[itest, :]\n",
    "y_train2 = gssdata2['poorhealth'].iloc[itrain]\n",
    "y_test2 = gssdata2['poorhealth'].iloc[itest]\n",
    "\n",
    "y_train.shape, X_train.shape, y_test.shape, X_test.shape\n",
    "\n",
    "logit2 = LogisticRegression(C=1000000)\n",
    "logit2.fit(X_train2, y_train2) \n",
    "print(logit2.score(X_test2,y_test2))\n",
    "logit2.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "#your code here: create confusion tables\n",
    "#####################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# your code here: create an ROC curve\n",
    "#####################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missingness approach \\#1: impute with a model (linear regression here) with uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use the dataset without NAs to build a model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "gssdata_full.head()\n",
    "\n",
    "X_imp = gssdata_full[['age','educ','female','partyid_dem','partyid_rep']]\n",
    "y_imp = gssdata_full['income']\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regress = LinearRegression()\n",
    "regress.fit(X_imp,y_imp)\n",
    "y_hat = regress.predict(X_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# your code here: \n",
    "# 1. figure out which observations have missing values for income,\n",
    "# 2. create the values you will use for imputation by:\n",
    "#  - calculating (1) the predicted yhats for the observations with missingness using the linear model \n",
    "#  - and then adding in error from the regression's residual distribution\n",
    "# 3. use these values to impute back into the income variable in the missing entries\n",
    "#####################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missing_index = gssdata.income[gssdata.income.isnull()].index\n",
    "missing_series = pd.Series(data = y_missing_noise, index = missing_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#back to the data set with missingness and impute the predictions\n",
    "gssdata_imp = gssdata.copy()\n",
    "\n",
    "gssdata_imp['income'] = gssdata_imp['income'].fillna(missing_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = plt.hist(gssdata_imp['income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsstemp = gssdata_imp[['age','educ','female','partyid_dem','partyid_rep','income']]\n",
    "X_train3 = gsstemp.iloc[itrain, :]\n",
    "X_test3 = gsstemp.iloc[itest, :]\n",
    "#y_train3 = gssdata_imp['poorhealth'].iloc[itrain]\n",
    "#y_test3 = gssdata_imp['poorhealth'].iloc[itest]\n",
    "\n",
    "y_train.shape, X_train.shape, y_test.shape, X_test.shape\n",
    "\n",
    "logit3 = LogisticRegression(C=1000000)\n",
    "logit3.fit(X_train3, y_train2) \n",
    "print(logit3.score(X_test3,y_test2))\n",
    "logit3.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test2,t_repredict(logit3, 0.5, X_test3)))\n",
    "print(confusion_matrix(y_train2,t_repredict(logit3, 0.5, X_train3)))\n",
    "\n",
    "print(confusion_matrix(y_test2,t_repredict(logit3, 0.1, X_test3)))\n",
    "print(confusion_matrix(y_train2,t_repredict(logit3, 0.1, X_train3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"poster\")\n",
    "ax=make_roc(\"logistic\",logit3, y_test2, X_test3, labe=10, skip=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
