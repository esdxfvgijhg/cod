{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS 109A/STAT 121A/AC 209A/CSCI E-109A \n",
    "\n",
    "# Lab 5: Regularization\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Fall 2017**<br>\n",
    "**Instructors: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "\n",
    "<ol start=\"0\">\n",
    "<li> Learning Goals </li>\n",
    "<li> Introduction to regularized regression </li>\n",
    "<li> Ridge regression with one predictor on a grid</li>\n",
    "<li> Ridge regression with polynomial features on a grid</li>\n",
    "<li> Cross-validation </li>\n",
    "<li> Refitting on full training set </li>\n",
    "</ol>\n",
    "**END OF LAB**\n",
    "<ol start=\"6\">\n",
    "<li> Feature selection with LASSO regression - good for homework 4!</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Learning Goals \n",
    "In this lab we continue where we left off in Lab 4, with regularized regression.  By the end of this lab, you will be able to:\n",
    "\n",
    "- Implement ridge and LASSO regression using `sklearn`.\n",
    "- Interpret the results of ridge and LASSO regression, and compare to the results from simple and multiple linear regression.\n",
    "\n",
    "*This lab maps on to lectures 7 and 8 and homework 4.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn.apionly as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  Introduction to regularized regression\n",
    "\n",
    "\n",
    "Recall from lecture the main idea of regularization.  In the ordinary least squares problem we minimize the loss function \n",
    "\n",
    "\\begin{equation}\n",
    "L(\\mathbf{\\beta}) = \\frac{1}{n} \\sum_{i = 1}^n |y_i - \\mathbf{\\beta}^T \\mathbf{x}_i|^2,\n",
    "\\end{equation}\n",
    "\n",
    "to determine regression coefficients $\\mathbf{\\beta}$.  Here $y_i$ is the response variable for observation $i$, and $\\mathbf{x}_i$ is a vector from the predictor matrix  corresponding to observation $i$.\n",
    "\n",
    "\n",
    "\n",
    "The general idea behind regularization is to penalize the loss function to account for possibly very large values of the coefficients $\\mathbf \\beta$.  The  aforementioned optimization problem is then adjusted accordingly.  Instead of minimizing $L(\\mathbf{\\beta})$, we minimize the regularized loss function\n",
    "\n",
    "\\begin{equation}\n",
    "L_{\\mathrm{reg}}(\\mathbf{\\beta}) = L(\\mathbf{\\beta}) + \\lambda R(\\mathbf{\\beta}),\n",
    "\\end{equation}\n",
    "\n",
    "where $R(\\mathbf{\\beta})$ is a penalty function and $\\lambda$ is a scalar that weighs the relative importance of this penalty.  In this lab we will explore two regularized regression models, ridge and LASSO.  In ridge regression, the penalty function is the sum of the squares of the parameters, giving the regularized loss function\n",
    "\n",
    "\\begin{equation}\n",
    "L_{\\mathrm{Ridge}}(\\mathbf{\\beta}) = \\frac{1}{n} \\sum_{i = 1}^n |y_i - \\mathbf{\\beta}^T \\mathbf{x}_i|^2 + \\lambda \\sum_{j = 1}^d \\beta_j^2.\n",
    "\\end{equation}\n",
    "\n",
    "In LASSO regression the penalty function is the sum of the magnitudes of the parameters, leading to\n",
    "\n",
    "\\begin{equation}\n",
    "L_{\\mathrm{LASSO}}(\\mathbf{\\beta}) = \\frac{1}{n} \\sum_{i = 1}^n |y_i - \\mathbf{\\beta}^T \\mathbf{x}_i|^2 + \\lambda \\sum_{j = 1}^d |\\beta_j|.\n",
    "\\end{equation}\n",
    "\n",
    "We will show how these optimization problems can be solved with `sklearn` to determine the model parameters $\\mathbf \\beta$.  We will also show how to choose $\\lambda$ appropriately via cross-validation.\n",
    "\n",
    "Let's continue working with our data from last time. We load and split the data as in Lab 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"data/noisypopulation.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `x` and `y` are the predictor and measured response variables, and `f` is the true response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = df.f.values\n",
    "x = df.x.values\n",
    "y = df.y.values\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexes=np.sort(np.random.choice(x.shape[0], size=60, replace=False))\n",
    "samplex = x[indexes]\n",
    "samplef = f[indexes]\n",
    "sampley = y[indexes]\n",
    "sample_df=pd.DataFrame(dict(x=x[indexes],f=f[indexes],y=y[indexes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the sample data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "datasize=sample_df.shape[0]\n",
    "#split dataset using the index, as we have x,f, and y that we want to split.\n",
    "itrain, itest = train_test_split(np.arange(60),train_size=0.8)\n",
    "xtrain= sample_df.x[itrain].values\n",
    "ftrain = sample_df.f[itrain].values\n",
    "ytrain = sample_df.y[itrain].values\n",
    "xtest= sample_df.x[itest].values\n",
    "ftest = sample_df.f[itest].values\n",
    "ytest = sample_df.y[itest].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Ridge regression with one predictor on a grid\n",
    "\n",
    "To begin, we'll use `sklearn` to do simple linear regression on the sampled training data.  We'll then do ridge regression with the same data, setting the penalty parameter $\\lambda$ to zero.  Setting $\\lambda = 0$ reduces the ridge problem to the simple ordinary least squares problem, so we expect the results of these models to be identical.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the the ordinary least squares model\n",
    "simp_reg = LinearRegression()\n",
    "#fit the model to training data\n",
    "simp_reg.fit(xtrain.reshape(len(xtrain),1), ytrain)\n",
    "#save the beta coefficients\n",
    "beta0_sreg = simp_reg.intercept_\n",
    "beta1_sreg = simp_reg.coef_[0]\n",
    "#make predictions everywhere\n",
    "ypredict = lambda x : beta0_sreg + beta1_sreg*x\n",
    "print(\"(beta0, beta1) = (%f, %f)\" %(beta0_sreg, beta1_sreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the above $\\beta$ coefficients as a benchmark for comparision to ridge and LASSO methods.  Let's see that we get the same coefficients with ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, [here](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) is the ridge regression documentation.  Notice that the weight $\\lambda$ is referred to as `alpha` in the documentation.\n",
    "\n",
    "The snippet of code below implements the ridge regression with $\\lambda = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the ridge regression model with specified lambda, ie, alpha\n",
    "ridge_reg = Ridge(alpha = 0)\n",
    "#fit the model to training data\n",
    "ridge_reg.fit(xtrain.reshape(-1,1), ytrain)  #xtrain.reshape(-1,1) and xtrain.reshape(len(xtrain),1) are equivalent\n",
    "#save the beta coefficients\n",
    "beta0_ridge = ridge_reg.intercept_\n",
    "beta1_ridge = ridge_reg.coef_[0]\n",
    "#make predictions everywhere\n",
    "ypredict_ridge = ridge_reg.predict(x.reshape(-1,1))\n",
    "print(\"(beta0, beta1) = (%f, %f)\" %(beta0_ridge, beta1_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beta coefficients for linear and ridge regressions coincide for $\\lambda = 0$, as expected. We plot the data and fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors = sns.color_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot in-sample training data\n",
    "plt.plot(xtrain, ytrain, 's', alpha=0.3, ms=10, label=\"in-sample y (observed)\")\n",
    "#plot population data\n",
    "plt.plot(x, y, '.', alpha=0.8, label=\"population y\");\n",
    "plt.plot(x, f, color = colors[1], label=\"God function\")\n",
    "#plot simple linear regression fit\n",
    "plt.plot(x, ypredict(x), alpha=0.5, label=\"OLS\")\n",
    "#plot ridge regression fit\n",
    "plt.plot(x, ypredict_ridge, 'k.', lw = 1, alpha=0.3, label=\"ridge\")\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.legend(loc=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE:** Play around with the values of  $\\lambda$ in the ridge regression code.  Increase $\\lambda$ from 0 to .01, from 0.01 to 1, from 1 to 5.  What do you observe?  What happens as $\\lambda$ goes to $\\infty$?  \n",
    "\n",
    "> **YOUR DISCUSSION HERE:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Ridge regression with polynomial features on a grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll make a more complex model by adding polynomial features.  Instead of building the linear model $y = \\beta_0 + \\beta_1 x$, we build a polynomial model $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\ldots \\beta_d x^d$ for some $d$ to be determined (see Lab 4 for details on choosing hyper-parameter $d$).  This regression will be linear though, since we'll be treating  $x^2, \\ldots, x^d$ themselves as predictors in the linear model.  We did this in Lab 4 but it's worth a review.  \n",
    "\n",
    "We map $x$ to $1, x, x^2, \\ldots, x^d$, and then build a linear regression model on this linear function of polynomial features.  To do this, we use `sklearn` to build what is known as the *Vandermonde* matrix, the generalizaiton of the predictor matrix $X$ discussed in Lab 3. For example, if we have three observations\n",
    "\n",
    "\\begin{equation*}\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\\\\\n",
    "\\end{pmatrix}, \\end{equation*}\n",
    "\n",
    "and we want  polynomial features up to and including degree 4, we build the predictor matrix\n",
    "\n",
    "\\begin{equation*}\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "\\end{pmatrix} \\rightarrow X = \\begin{bmatrix}\n",
    "x_1^0 & x_1^1 & x_1^2 & x_1^3 & x_1^4\\\\\n",
    "x_2^0 & x_2^1 & x_2^2 & x_2^3 & x_2^4\\\\\n",
    "x_3^0 & x_3^1 & x_3^2 & x_3^3 & x_3^4\\\\\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "1& x_1^1 & x_1^2 & x_1^3 & x_1^4\\\\\n",
    "1 & x_2^1 & x_2^2 & x_2^3 & x_2^4\\\\\n",
    "1 & x_3^1 & x_3^2 & x_3^3 & x_3^4\\\\\n",
    "\\end{bmatrix}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE: ** Before we continue working with the data, make a toy vector called `toy`, where  \n",
    "\n",
    ">\\begin{equation}\n",
    "\\mathrm{toy} = \\begin{pmatrix}\n",
    "0 \\\\\n",
    "2 \\\\\n",
    "5 \\\\\n",
    "\\end{pmatrix}\n",
    ". \n",
    "\\end{equation}\n",
    "> Build the feature matrix up to (and including) degree 4. Confirm that the entries in the matrix are what you'd expect based on the above discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now continue working with our data.  We write a function to make polynomial features of given degrees as we did in Lab 4, and we store the features in a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_features(train_set, test_set, degrees):\n",
    "    train_dict = {}\n",
    "    test_dict = {}\n",
    "    for d in degrees:\n",
    "        traintestdict={}\n",
    "        train_dict[d] = PolynomialFeatures(d).fit_transform(train_set.reshape(-1,1))\n",
    "        test_dict[d] = PolynomialFeatures(d).fit_transform(test_set.reshape(-1,1))\n",
    "    return train_dict, test_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE: ** Fill in the code below to perform ridge regression on the training data for the given set of $\\lambda$.  Then predict on the grid and store the results in  `ypredict_ridge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 20\n",
    "rows = 7\n",
    "cols = 2\n",
    "lambdas = [0., 1e-6, 1e-3, 1e-2, 1e-1, 1, 10]\n",
    "grid_to_predict = np.arange(0, 1, .01)\n",
    "train_dict, grid_dict = make_features(xtrain, grid_to_predict, range(0,d + 1))\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(12, 24))\n",
    "axs = axs.ravel()\n",
    "Xtrain = train_dict[d]\n",
    "\n",
    "for i, lam in enumerate(lambdas):\n",
    "    #your code here\n",
    "\n",
    "    #code provided from here on\n",
    "    left = 2*i\n",
    "    right = 2*i + 1\n",
    "    axs[left].plot(xtrain, ytrain, 's', alpha=0.3, ms=10, label=\"in-sample y (observed)\")\n",
    "    axs[left].plot(x, y, '.', alpha=0.8, label=\"population y\")\n",
    "    axs[left].plot(grid_to_predict, ypredict_ridge, 'k-', label=\"lambda =  %s\" % str(lam))\n",
    "    axs[left].set_ylabel('$y$')\n",
    "    axs[left].set_ylim((0, 1))\n",
    "    axs[left].set_xlim((0, 1))\n",
    "    axs[left].legend(loc=2)\n",
    "    coef = ridge_reg.coef_.ravel()\n",
    "    axs[right].semilogy(np.abs(coef), marker='o', label=\"lambda = %s\" % str(lam))\n",
    "    axs[right].set_ylim((1e-1, 1e15))\n",
    "    axs[right].yaxis.set_label_position(\"right\")\n",
    "    axs[right].set_ylabel('abs(coefficient)')\n",
    "    axs[right].legend(loc='upper left')\n",
    "axs[2*(rows-1)].set_xlabel(\"x\")\n",
    "axs[2*(rows-1) + 1].set_xlabel(\"coefficients\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, as we increase $\\lambda$ from 0 to 1, we start out overfitting, then doing well, and then our fits develop a mind of their own irrespective of data, as the penalty term dominates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE:** What would you expect if you compared a performance metric between these models on a grid?\n",
    "\n",
    "> **YOUR DISCUSSION HERE:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Cross-validation\n",
    "\n",
    "Let's use cross-validation to determine the critical value of $\\lambda$, which we'll refer to as $\\lambda^*$. To do this we use the concept of a *meta-estimator* from scikit-learn.  As the API paper from Lab 4 explains:\n",
    "\n",
    ">In scikit-learn, model selection is supported in two distinct meta-estimators, GridSearchCV and RandomizedSearchCV. They take as input an estimator (basic or composite), whose hyper-parameters must be optimized, and a set of hyperparameter settings to search through.\n",
    "\n",
    "The concept of a meta-estimator allows us to wrap, for example, cross-validation, or methods that build and combine simpler models or schemes. For example:\n",
    "\n",
    "    est = Ridge()\n",
    "    parameters = {\"alpha\": [1e-8, 1e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 1e-2, 1e-1, 1.0]}\n",
    "    gridclassifier=GridSearchCV(est, param_grid=parameters, cv=4, scoring=\"neg_mean_squared_error\")\n",
    "    \n",
    "The `GridSearchCV` replaces the manual iteration over the folds using `KFolds` and the averaging we did in Lab 4, doing it all for us. It takes a hyper-parameter grid in the shape of a dictionary as input, and sets $\\lambda$ to the values you want to try, one by one. It then trains the model using cross-validation, and gets the error for each value of the hyper-parameter $\\lambda$. Finally it compares the errors for the different $\\lambda$'s, and picks the best choice model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "def cv_optimize_ridge(x, y, list_of_lambdas, n_folds=4):\n",
    "    est = Ridge()\n",
    "    parameters = {'alpha': list_of_lambdas}\n",
    "    #the scoring parameter below is the default one in ridge, but you can use a different one\n",
    "    #in the cross-validation phase if you want.\n",
    "    gs = GridSearchCV(est, param_grid=parameters, cv=n_folds, scoring=\"neg_mean_squared_error\")\n",
    "    gs.fit(x, y)\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE:** Use the function above to fit the model on the training set with 4-fold cross validation.  Save the fit as the variable `fitmodel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitmodel.best_estimator_, fitmodel.best_params_, fitmodel.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also output the mean cross-validation error at different $\\lambda$ (with a negative sign, as scikit-learn likes to maximize negative error which is equivalent to minimizing error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitmodel.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit_lambdas = [d['alpha'] for d in fitmodel.cv_results_['params']]\n",
    "fit_scores = fitmodel.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE:** Plot log10-log10 plot of `-fit_scores` versus `fit_lambdas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Refitting on full training set\n",
    "\n",
    "We now refit the estimator on the training set, and calculate and plot the test set error and the polynomial coefficients. Notice how many of these coefficients have been pushed to lower values or 0.\n",
    "\n",
    "\n",
    "> **EXERCISE:** Assign to variable `est` the classifier obtained by fitting the entire training set using the best $\\lambda$ found above.  Assign the predictions to the variable `ypredict_ridge_best`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code provided from here on\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "left = 0\n",
    "right = 1\n",
    "axs[left].plot(xtrain, ytrain, 's', alpha=0.3, ms=10, label=\"in-sample y (observed)\")\n",
    "axs[left].plot(x, y, '.', alpha=0.8, label=\"population y\")\n",
    "axs[left].plot(grid_to_predict, ypredict_ridge_best, 'k-', label=\"lambda =  %s\" % str(lambdawechoose))\n",
    "axs[left].set_ylabel('$y$')\n",
    "axs[left].set_ylim((0, 1))\n",
    "axs[left].set_xlim((0, 1))\n",
    "axs[left].legend(loc=2)\n",
    "coef = est.coef_.ravel()\n",
    "axs[right].semilogy(np.abs(coef), marker='o', label=\"lambda = %s\" % str(lambdawechoose))\n",
    "axs[right].set_ylim((1e-1, 1e15))\n",
    "axs[right].yaxis.set_label_position(\"right\")\n",
    "axs[right].set_ylabel('abs(coefficient)')\n",
    "axs[right].legend(loc='upper left')\n",
    "axs[left].set_xlabel(\"x\")\n",
    "axs[right].set_xlabel(\"coefficients\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF LAB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Feature selection with LASSO regression \n",
    "Below is a completely worked example of feature selection with LASSO, which will be helpful for homework 4.  For reference [here](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) is the documentation for LASSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "#function to do lasso with cross validation\n",
    "def cv_optimize_lasso(X, y, list_of_lambdas, n_folds=4):\n",
    "    #build the lasso model\n",
    "    clf = Lasso()\n",
    "    parameters = {\"alpha\": list_of_lambdas}\n",
    "    #the scoring parameter below is the default one in ridge, but you can use a \n",
    "    #different one in the cross-validation phase if desired.\n",
    "    gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, scoring=\"neg_mean_squared_error\")\n",
    "    gs.fit(X, y)\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of Lambda (lol!) values\n",
    "lol = [1e-8,1e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0]\n",
    "#fit lasso model to training data with cross-validation\n",
    "fitmodel_lasso = cv_optimize_lasso(Xtrain, ytrain, lol, n_folds=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose the optimal lambda\n",
    "lambdawechoose_lasso = fitmodel_lasso.best_params_['alpha']\n",
    "#estimate with this optimal lambda\n",
    "est_lasso = Lasso(alpha=lambdawechoose_lasso).fit(Xtrain,ytrain)\n",
    "est_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function that pulls out the important features\n",
    "def nonzero_lasso(est, lcols):\n",
    "    featuremask=(est.coef_ !=0.0)\n",
    "    return pd.DataFrame(dict(feature=lcols, coef=est.coef_, \n",
    "        abscoef=np.abs(est.coef_)))[featuremask].sort_values('abscoef', \n",
    "                                                    ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x^1, x^2, x^6, x^20 are the important features\n",
    "lasso_importances=nonzero_lasso(est_lasso, list(range(d+1)))\n",
    "lasso_importances.set_index(\"feature\", inplace=True)\n",
    "lasso_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function that pulls out the trivial features\n",
    "def zero_lasso(est, lcols):\n",
    "    featuremask=(est.coef_ ==0.0)\n",
    "    return pd.DataFrame(dict(feature=lcols, coef=est.coef_, \n",
    "        abscoef=np.abs(est.coef_)))[featuremask].sort_values('abscoef', \n",
    "                                                    ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate and print the trivial features.  \n",
    "lasso_zeros=zero_lasso(est_lasso, list(range(d+1)))\n",
    "lasso_zeros.set_index(\"feature\", inplace=True)\n",
    "lasso_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17 of the 21 features are trivial, 4 are important\n",
    "len(lasso_zeros), len(lasso_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
