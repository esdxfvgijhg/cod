{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS 109A/STAT 121A/AC 209A/CSCI E-109A \n",
    "\n",
    "# Lab 8: Discriminant Analysis \n",
    "\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Fall 2017**<br>\n",
    "**Instructors: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "\n",
    "<ol start=\"0\">\n",
    "<li> Learning Goals </li>\n",
    "<li> Temporal patterns in urban demographics </li>\n",
    "<li> Geographic patterns in urban demographics </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Learning Goals \n",
    "\n",
    "In this lab we'll work with demographics of a region of the cities of Pavlopolis and Kevinsville from the years 2000 to 2010.  We'll use the data to predict household economic status from its geographical location.\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Implement different classifiers and calculate predictive accuracy of these classifiers,\n",
    "\n",
    "- Compare different classifiers for general predictive accuracy and computational efficiency.\n",
    "\n",
    "\n",
    "*This lab maps on to lectures 13 and 14 and homework 7. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy.stats import mode\n",
    "from sklearn import linear_model\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import discriminant_analysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.cross_validation import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Predicting Urban Demographic Changes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: Temporal patterns in urban demographics\n",
    "For this first part of lab, we aim to build a model for the city of Pavlopolis that accurately predicts household economic status from its geographical location.  We want the model to be robust, that is, the model should be accurate over many years (through 2010 and beyond). \n",
    "\n",
    "The data is contained in `dataset_1_year_2000.txt`, ..., `dataset_1_year_2010.txt`. The first two columns of each dataset are the adjusted latitude and longitude of some randomly sampled houses. The last column contains economic status of a household: \n",
    "\n",
    "0: low-income, \n",
    "\n",
    "1: middle-class, \n",
    "\n",
    "2: high-income \n",
    "\n",
    "Due to the migration of people in and out of the city, the distribution of each economic group over this region changes over the years. The city of Pavlopolis estimates that in this region there is approximately a 25% yearly increase in high-income households; and a 25% decrease in the remaining population, with the decrease being roughly the same amongst both the middle class and lower income households.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "We start by importing and visualizing the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load one file, check the data type, check that the data matches the description above\n",
    "data2000_glance = np.loadtxt('datasets/dataset_1_year_2000.txt')\n",
    "print(type(data2000_glance))\n",
    "data2000_glance[:10, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE:** Fill in the code below to load the data and separate the predictors (variable `x`) and response (variable `y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize data for every other year\n",
    "fig, ax = plt.subplots(2, 3, figsize = (12, 5))\n",
    "\n",
    "#Create index for subplots\n",
    "ind = 0\n",
    "#Iterate from year 0 to 10, with steps of 2\n",
    "for i in range(0, 11, 2):\n",
    "    \n",
    "    #your code begins here\n",
    "    #Load data\n",
    "    \n",
    "    #Split into predictor/response\n",
    "\n",
    "    #your code ends here\n",
    "\n",
    "    #Plot each class for the current year in different colors         \n",
    "    ax[ind // 3, ind % 3].scatter(x[y == 2, 0], x[y == 2, 1], color='green', alpha=0.4,\n",
    "                                 label = '2-High income')\n",
    "    ax[ind // 3, ind % 3].scatter(x[y == 1, 0], x[y == 1 ,1], \n",
    "                                 color='blue', alpha=0.4,\n",
    "                                 label = '1-Middle class')\n",
    "    ax[ind // 3, ind % 3].scatter(x[y == 0, 0], x[y == 0, 1], \n",
    "                                 color='red', alpha=0.4,\n",
    "                                 label = '0-Low income')\n",
    "    \n",
    "    # LABEL AXIS, TITLE\n",
    "    ax[ind // 3, ind % 3].set_xlabel('Latitude')\n",
    "    ax[ind // 3, ind % 3].set_ylabel('Longitude')\n",
    "    ax[ind // 3, ind % 3].set_title(str(2000 + i))\n",
    "    \n",
    "    #Update index\n",
    "    ind = ind + 1\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE:** What do you observe from the data visualizations?\n",
    "\n",
    "> **YOUR DISCUSSION HERE:** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model comparison\n",
    "\n",
    "Let's  compare the the following four classification schemes for addressing the problem of predicting household economic status from geographic location:\n",
    "\n",
    "1. Training a  logistic regression model on one year and use it repeatedly\n",
    "2. Training a Q/LDA model on one year and use it repeatedly\n",
    "3. Training a KNN model on one year and use it repeatedly\n",
    "4. Training a Q/LDA model yearly\n",
    "\n",
    "We'll consider the following \"goodness\" factors:\n",
    "\n",
    "1. general predictive accuracy\n",
    "2. computational efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE:** Of the four models above, which ones do you believe are most computationally efficient?\n",
    "\n",
    "> **YOUR DISCUSSION HERE:** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now train three single models  (logistic regression, LDA, and KNN) once, and explore the accuracy of these models over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE:** Fill in the code below to fit a logistic regression (variable `logreg`), KNN (variable `knn`), and lda (variable `lda`) models on the 2000 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data for year 2000\n",
    "data = np.loadtxt('datasets/dataset_1_year_2000.txt')\n",
    "\n",
    "#Split predictors, response\n",
    "x = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "#your code begins here\n",
    "#Fit a logistic regression model on year 2000 data\n",
    "\n",
    "\n",
    "#Fit a lda model on year 2000 data\n",
    "\n",
    "\n",
    "#Fit a knn model on year 2000 data\n",
    "\n",
    "\n",
    "#your code ends here\n",
    "\n",
    "acc_log = []\n",
    "acc_lda = []\n",
    "acc_knn = []\n",
    "\n",
    "#Iterate through all the years\n",
    "for i in range(1, 11):\n",
    "    #Load data for year 2000 + i \n",
    "    data_i = np.loadtxt('datasets/dataset_1_year_' + str(2000 + i) + '.txt')\n",
    "    \n",
    "    #Split predictors, response\n",
    "    x_i = data_i[:, :-1]\n",
    "    y_i = data_i[:, -1]\n",
    "\n",
    "    #Compute predictive accuracies of various models trained on 2000 data\n",
    "    acc_log.append(logreg.score(x_i, y_i))\n",
    "    acc_lda.append(lda.score(x_i, y_i))\n",
    "    acc_knn.append(knn.score(x_i, y_i))\n",
    "            \n",
    "#Plot accuracy over years\n",
    "years = np.arange(1,11) + 2000 # x-axis is years 2001-2010\n",
    "width = 0.25 # width of bar\n",
    "\n",
    "plt.figure(figsize = (10,3))\n",
    "plt.bar(years, acc_log, width, color='blue', alpha=0.5, label='LogReg')\n",
    "plt.bar(years + width, acc_lda, width, color='red', alpha=0.5, label='LDA')\n",
    "plt.bar(years + 2 * width, acc_knn, width, color='green', alpha=0.5, label='KNN, k=5')\n",
    "\n",
    "#Labels\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Prediction Accuracy')\n",
    "plt.legend(loc = 'upper left', bbox_to_anchor=(1,1)) # legend upper left outside\n",
    "plt.xticks(years + width + 0.125, years); # set x-ticks spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE:** Are these three models robust?  That is, do these models perform well over the years?\n",
    "\n",
    "> **YOUR DISCUSSION HERE:** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now consider the remaining modeling option:\n",
    "\n",
    "1. Training a Q/LDA model yearly\n",
    "\n",
    "\n",
    "\n",
    "What does it mean to train a QDA or LDA model for each year? Recall that Q/LDA classifiers first model the distribution of the data within each class using a normal distribution, $\\mathcal{N}(\\mu, \\Sigma)$. Then the Q/LDA models the distribution of the data amongst the classes. For us, since we have three classes, this means that our \"model\" consists of the following 9 pieces of information (three for each class, two to define the distribution of data in the class and one to define the class proportion):\n",
    "\n",
    "1. Class 0: \n",
    "   - Distribution: $p(x | y = 0) = \\mathcal{N}(\\mu_0, \\Sigma_0)$\n",
    "   - Proportion: $p(y=0)=\\pi_0 = \\frac{\\text{data in Class 0}}{\\text{total number of data}}$\n",
    "\n",
    "2. Class 1: \n",
    "   - Distribution: $p(x | y = 1) = \\mathcal{N}(\\mu_1, \\Sigma_1)$\n",
    "   - Proportion: $p(y=1)=\\pi_1 = \\frac{\\text{data in Class 1}}{\\text{total number of data}}$\n",
    "   \n",
    "3. Class 2: \n",
    "   - Distribution: $p(x | y = 2) = \\mathcal{N}(\\mu_2, \\Sigma_2)$\n",
    "   - Proportion: $p(y=2)=\\pi_2 = \\frac{\\text{data in Class 2}}{\\text{total number of data}}$\n",
    "   \n",
    "Retraining a Q/LDA model means re-estimating $\\mu, \\Sigma$ and $\\pi$ for each class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE:** At the beginning of lab we were given information on the projected yearly growth rate of the classes, so we don't need the entire set of population data to re-estimate $\\pi$ for each class.  Use this information to write down estimates for $\\pi_2^{\\text{year i + 1}}$.  Then, assuming that the total population is approximately constant over the years, estimate $\\pi_0^{\\text{year i + 1}}$ and $\\pi_1^{\\text{year i + 1}}$.\n",
    "\n",
    "> **YOUR DISCUSSION HERE:** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, if the distribution of the data within each class changes (i.e. $\\mu$ and $\\Sigma$ changes within each class) then we will need re-estimate them using new data each year. Let's check, using data visualization, to see if this is indeed the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize data for every other year\n",
    "fig, ax = plt.subplots(3, 6, figsize = (23, 10))\n",
    "\n",
    "#Iterate from year 0 to 10, with steps of 2\n",
    "for i in range(0, 11, 2):\n",
    "    #Load data\n",
    "    data = np.loadtxt('datasets/dataset_1_year_' + str(2000 + i) + '.txt')\n",
    "    \n",
    "    #Split into predictor/response\n",
    "    x = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "    \n",
    "    \n",
    "    #Plot each class for the current year in different colors\n",
    "    ax[0, i // 2].scatter(x[y == 2, 0], x[y == 2, 1],\n",
    "                         color='g', alpha=0.6,\n",
    "                         label = '2-High income')\n",
    "    ax[1, i // 2].scatter(x[y == 1, 0], x[y == 1 ,1], \n",
    "                         color='b', alpha=0.6,\n",
    "                         label = '1-Middle class')\n",
    "    ax[2, i // 2].scatter(x[y == 0, 0], x[y == 0, 1], \n",
    "                         color='r', alpha=0.6,\n",
    "                         label = '0-Low income')\n",
    "    \n",
    "    #Labels\n",
    "    ax[0, i // 2].set_xlabel('Latitude')\n",
    "    ax[0, i // 2].set_ylabel('Longitude')\n",
    "    ax[0, i // 2].set_title(str(2000 + i))\n",
    "    \n",
    "    #Labels\n",
    "    ax[1, i // 2].set_xlabel('Latitude')\n",
    "    ax[1, i // 2].set_ylabel('Longitude')\n",
    "    ax[1, i // 2].set_title(str(2000 + i))\n",
    "    \n",
    "    # LABEL AXIS, TITLE\n",
    "    ax[2, i // 2].set_xlabel('Latitude')\n",
    "    ax[2, i // 2].set_ylabel('Longitude')\n",
    "    ax[2, i // 2].set_title(str(2000 + i))\n",
    "\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **EXERCISE:** Comment on how the distribution in each class appears to change over the years.  Does this effect the efficiency of Q/LDA models?\n",
    "\n",
    "> **YOUR DISCUSSION HERE:** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "So how efficient is our Q/LDA model? We train the model once on a single year's worth of data. Then, for any subsequent year, the update is just a few lines of arithmetic. For example, say we train our Q/LDA model on 2000 data, given a point from 2001, $x^{2001}$, we predict the class label for $x^{2001}$ by:\n",
    "\n",
    "1. find the probability that $x^{2001}$ will be labeled Class 0 to 2 using our model from 2000:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p_{2000}(y=0 | x^{2001}) &= p_{2000}(x^{2001} | y=0) p_{2000}(y=0) = \\mathcal{N}(x^{2001}; \\mu_0, \\Sigma_0) \\pi_0^{2000}\\\\\n",
    "p_{2000}(y=1 | x^{2001}) &= p_{2000}(x^{2001} | y=1) p_{2000}(y=1) = \\mathcal{N}(x^{2001}; \\mu_1, \\Sigma_1) \\pi_1^{2000}\\\\\n",
    "p_{2000}(y=2 | x^{2001}) &= p_{2000}(x^{2001} | y=2) p_{2000}(y=2) = \\mathcal{N}(x^{2001}; \\mu_2, \\Sigma_2) \\pi_2^{2000}\n",
    "\\end{aligned}\n",
    "$$\n",
    "2. adjust the probabilities predicted using the 2000 model, by updating $\\pi_i$ with 2001 class proportions:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p_{2001}(y=0 | x^{2001}) &= p_{2000}(y=0 | x^{2001}) \\frac{\\pi_0^{2001}}{\\pi_0^{2000}}\\\\\n",
    "p_{2001}(y=1 | x^{2001}) &= p_{2000}(y=1 | x^{2001}) \\frac{\\pi_1^{2001}}{\\pi_1^{2000}}\\\\\n",
    "p_{2001}(y=2 | x^{2001}) &= p_{2000}(y=2 | x^{2001}) \\frac{\\pi_2^{2001}}{\\pi_2^{2000}}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "3. take the maximum of $\\{p_{2001}(y=0 | x^{2001}), p_{2001}(y=1 | x^{2001}), p_{2001}(y=2 | x^{2001}) \\}$ and predict the class label corresponding to the maximum probability.\n",
    "\n",
    "In terms of efficiency, clearly updating a Q/LDA model is cheaper than retraining a KNN or logistic regression each year. Let's see if the accuracy is comparable.\n",
    "\n",
    "> **EXERCISE: ** Fill in the code below to re-estimate the class proportions (see equations from two exercises ago)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data for year 2000\n",
    "data = np.loadtxt('datasets/dataset_1_year_2000.txt')\n",
    "\n",
    "#Split predictors, response\n",
    "x = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "#your code begins here\n",
    "#Fit a lda model on year 2000 data\n",
    "\n",
    "#your code ends here\n",
    "\n",
    "acc_lda_corrected = [] # Store corrected LDA accuracies\n",
    "\n",
    "#Store class proportions in 2000\n",
    "p0_2000 = (y==0).mean()\n",
    "p1_2000 = (y==1).mean()\n",
    "p2_2000 = (y==2).mean()\n",
    "\n",
    "#Store class proportions in current year\n",
    "p0_current = (y==0).mean()\n",
    "p1_current = (y==1).mean()\n",
    "p2_current = (y==2).mean()\n",
    "\n",
    "for k in range(1, 11):\n",
    "    #Load data\n",
    "    data_i = np.loadtxt('datasets/dataset_1_year_' + str(2000 + k) + '.txt')\n",
    "    \n",
    "    #Split into predictor, response\n",
    "    x_i = data_i[:, :-1]\n",
    "    y_i = data_i[:, -1]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_i, y_i, test_size=0.30, random_state=0)\n",
    "    \n",
    "    #your code begins here\n",
    "    #Re-estimate class proportions (25% increase in p2, adjust p0, p1 accordingly)\n",
    "\n",
    "    #your code ends here\n",
    "    \n",
    "    #Re-estimate class label probabilities \n",
    "    pred_logprob = lda.predict_log_proba(x_test) # compute log-probabilities\n",
    "    pred_logprob[:, 0] = pred_logprob[:, 0] + np.log(p0_current / p0_2000) # correction for class 0\n",
    "    pred_logprob[:, 1] = pred_logprob[:, 1] + np.log(p1_current / p1_2000) # correction for class 1\n",
    "    pred_logprob[:, 2] = pred_logprob[:, 2] + np.log(p2_current / p2_2000) # correction for class 2\n",
    "\n",
    "    \n",
    "    #Predict class label using re-estimated probabilities\n",
    "    y_pred = pred_logprob.argmax(axis = 1)\n",
    "    \n",
    "    #Compute accuracy \n",
    "    acc_lda_corrected.append(np.mean(y_test == y_pred))\n",
    "    \n",
    "#Plot accuracy over years\n",
    "years = np.arange(1, 11) + 2000 # x-axis is years 2001-2010\n",
    "width = 0.25 # width of bar\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.bar(years + width, acc_lda, width, color='red', alpha=0.5, label='LDA')\n",
    "plt.bar(years + 2 * width, acc_lda_corrected, width, color='green', alpha=0.5, label='LDA yearly')\n",
    "\n",
    "\n",
    "\n",
    "#Labels\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Prediction Accuracy')\n",
    "plt.legend(loc = 'upper left', bbox_to_anchor=(1, 1)) # legend upper left outside\n",
    "plt.title('Overall predictive accuracy of LDA models')\n",
    "plt.xticks(years + width + 0.125, years); # set x-ticks spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to training a model once and using it repeatedly, training the Q/LDA yearly performs better in terms of predictive accuracy.  It isn't significantly more expensive though.\n",
    "\n",
    "** Note:** Why LDA and not QDA? Well since we've already observed that the distributions of data within the three classes are very simliar (in terms of shape) we can assume that the covariance matrices for all three classes will be the same - hence our model is LDA rather than QDA\n",
    "\n",
    "** Conclusions: **  Based on the data visualization from the first 5 years, it's clear that while the class proportions are changing, the class distributions are not (the center and shape of the three classes are constant through time). Thus, updating our LDA on a yearly basis is simple, as this just requires us to adjust our estimates of the class proportions $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Geographic patterns in urban demographics\n",
    "In `dataset_2.txt` and `dataset_3.txt` you have the demographic information for a random sample of houses in two regions in Kevinsville. There are only two economic brackets for the households in these datasets: \n",
    "\n",
    "0: low-income or middle-class, \n",
    "\n",
    "1: high-income. \n",
    "\n",
    "The data for each region is shown in the visualizations below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "data = np.loadtxt('datasets/dataset_2.txt')\n",
    "x = data[:,0:-1]\n",
    "y = data[:,-1]\n",
    "\n",
    "# Plot data\n",
    "ax[0].scatter(x[y == 1, 0], x[y == 1, 1], color='b', alpha=0.2)\n",
    "ax[0].scatter(x[y == 0, 0], x[y == 0, 1], color='r', alpha=0.2)\n",
    "\n",
    "# Label axes, set title\n",
    "ax[0].set_title('dataset_2')\n",
    "ax[0].set_xlabel('Latitude')\n",
    "ax[0].set_ylabel('Longitude')\n",
    "\n",
    "data = np.loadtxt('datasets/dataset_3.txt')\n",
    "x = data[:,0:-1]\n",
    "y = data[:,-1]\n",
    "\n",
    "# Plot data\n",
    "ax[1].scatter(x[y == 1, 0], x[y == 1, 1], color='b', alpha=0.2)\n",
    "ax[1].scatter(x[y == 0, 0], x[y == 0, 1], color='r', alpha=0.2)\n",
    "\n",
    "# Label axes, set title\n",
    "ax[1].set_title('dataset_3')\n",
    "ax[1].set_xlabel('Latitude')\n",
    "ax[1].set_ylabel('Longitude')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">** EXERCISE:** For each region, comment on the appropriateness of using KNN for classification.\n",
    "\n",
    "> **YOUR DISCUSSION HERE:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is code for visualizing decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------  plot_decision_boundary\n",
    "# A function that visualizes the data and the decision boundaries\n",
    "# Input: \n",
    "#      x (predictors)\n",
    "#      y (labels)\n",
    "#      poly_flag (a boolean parameter, fits quadratic model if true, otherwise linear)\n",
    "#      title (title for plot)\n",
    "#      ax (a set of axes to plot on)\n",
    "# Returns: \n",
    "#      ax (axes with data and decision boundaries)\n",
    "\n",
    "def plot_decision_boundary(x, y, model, poly_flag, title, ax):\n",
    "    # Plot data\n",
    "    ax.scatter(x[y == 1, 0], x[y == 1, 1], c='blue', label='Normal', cmap=plt.cm.coolwarm)\n",
    "    ax.scatter(x[y == 0, 0], x[y == 0, 1], c='red', label='Normal', cmap=plt.cm.coolwarm)\n",
    "    \n",
    "    # Create mesh\n",
    "    interval = np.arange(0,1,0.01)\n",
    "    n = np.size(interval)\n",
    "    x1, x2 = np.meshgrid(interval, interval)\n",
    "    x1 = x1.reshape(-1, 1)\n",
    "    x2 = x2.reshape(-1, 1)\n",
    "    xx = np.concatenate((x1, x2), axis=1)\n",
    "\n",
    "    # Predict on mesh points\n",
    "    if(poly_flag):\n",
    "        quad_features = preprocessing.PolynomialFeatures(degree=2)\n",
    "        xx = quad_features.fit_transform(xx)\n",
    "    yy = model.predict(xx)    \n",
    "    yy = yy.reshape((n, n))\n",
    "\n",
    "    # Plot decision surface\n",
    "    x1 = x1.reshape(n, n)\n",
    "    x2 = x2.reshape(n, n)\n",
    "    ax.contourf(x1, x2, yy, cmap=plt.cm.coolwarm, alpha=0.1)\n",
    "\n",
    "    \n",
    "    # Label axes, set title\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Latitude')\n",
    "    ax.set_ylabel('Longitude')\n",
    "    \n",
    "    return ax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------  fit_and_plot_models\n",
    "# A function that visualizes the data and the decision boundaries\n",
    "# Input: \n",
    "#      file_name (name of the file containing dataset)\n",
    "# Returns: \n",
    "#      None\n",
    "\n",
    "def fit_and_plot_models(file_name): \n",
    "    data = np.loadtxt(file_name)\n",
    "    x = data[:,0:-1]\n",
    "    y = data[:,-1]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(15, 3))\n",
    "\n",
    "    # Plain Logistic Regression\n",
    "    logreg = linear_model.LogisticRegression()\n",
    "    logreg.fit(x, y)\n",
    "    acc_logreg = logreg.score(x, y)\n",
    "\n",
    "    str_title = 'LogReg (acc = ' + str(acc_logreg) + ')'\n",
    "    ax[0] = plot_decision_boundary(x, y, logreg, False, str_title, ax[0])\n",
    "\n",
    "    # LDA\n",
    "    lda = discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "    lda.fit(x, y)\n",
    "    acc_lda = lda.score(x, y)\n",
    "\n",
    "    str_title = 'LDA (acc = ' + str(acc_lda) + ')'\n",
    "    ax[1] = plot_decision_boundary(x, y, lda, False, str_title, ax[1])\n",
    "\n",
    "    # QDA\n",
    "    qda = discriminant_analysis.QuadraticDiscriminantAnalysis()\n",
    "    qda.fit(x, y)\n",
    "    acc_qda = qda.score(x, y)\n",
    "\n",
    "    str_title = 'QDA (acc = ' + str(acc_qda) + ')'\n",
    "    ax[2] = plot_decision_boundary(x, y, qda, False, str_title, ax[2])\n",
    "\n",
    "    # Logistic Regression with Quadratic Terms\n",
    "    quad_features = preprocessing.PolynomialFeatures(degree = 2)\n",
    "    x_expanded = quad_features.fit_transform(x)\n",
    "    logreg_poly = linear_model.LogisticRegression(C=1000)\n",
    "    logreg_poly.fit(x_expanded, y)\n",
    "    acc_logreg_poly = logreg_poly.score(x_expanded, y)\n",
    "    \n",
    "    str_title = 'LogReg-poly (acc = ' + str(acc_logreg_poly) + ')'\n",
    "    ax[3] = plot_decision_boundary(x, y, logreg_poly, True, str_title, ax[3])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">** EXERCISE:** Choose between Q/LDA and logistic regression by visualizing various types decision boundaries.  Explore and discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **YOUR DISCUSSION HERE:** "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
