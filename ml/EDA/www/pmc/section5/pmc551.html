
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">

<HTML>


<!-- Mirrored from www.itl.nist.gov/div898/handbook/pmc/section5/pmc551.htm by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 17 Feb 2017 22:22:06 GMT -->
<HEAD>
<script async type="text/javascript"
        id="_fed_an_ua_tag"
        src="https://dap.digitalgov.gov/Universal-Federated-Analytics-Min.js?agency=DOC&amp;subagency=NIST&amp;pua=UA-37115410-50&amp;yt=true&amp;exts=ppsx,pps,f90,sch,rtf,wrl,txz,m1v,xlsm,msi,xsd,f,tif,eps,mpg,xml,pl,xlt,c">
</script>
<script type="text/javascript"
            src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default.js">
</script>
<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1">
<META NAME="GENERATOR" CONTENT="Mozilla/4.05 [en] (WinNT; U) [Netscape]">
<TITLE>6.5.5.1. Properties of Principal Components</TITLE>
</HEAD>

<BODY BGCOLOR="FFFFCC">

<IMG SRC="../../gifs/nvgtbr.gif" BORDER=0 VALIGN="TOP" ISMAP USEMAP="#MenuBar">
<map name="MenuBar">
<area shape="rect" alt="Next Page" href="pmc552.html" coords="463,27,504,45">
<area shape="rect" alt="Previous Page" href="pmc55.html" coords="417,28,459,45">
<area shape="rect" alt="Home" href="../../index-2.html" coords="52,0,100,43">
<area shape="rect" alt="Tools & Aids" href="../../toolaids.html" coords="165,27,264,46">
<area shape="rect" alt="Search Handbook" href="../../search.html" coords="307,28,366,44">
<area shape="default" nohref>
</map>
<BR>

<TABLE CELLSPACING=20 CELLPADDING=0 WIDTH=540>

<TR>
<TD VALIGN=TOP COLSPAN=2>
<FONT SIZE=-1>
<FONT COLOR="#D60021">6.</FONT>
<FONT COLOR="#00105A"><A HREF="../pmc.html">Process or Product Monitoring and Control</a></FONT>
<BR>
<FONT COLOR="#D60021">6.5.</FONT>
<FONT COLOR="#00105A"><A HREF="pmc5.html">Tutorials</a></FONT>
<BR>
<FONT COLOR="#D60021">6.5.5.</FONT>
<FONT COLOR="#00105A"><A HREF="pmc55.html">Principal Components</a></FONT>
<BR>
</FONT>
<BR>
<TABLE>
<TR>
<TD VALIGN=top>
<H2><FONT COLOR="#D60021">6.5.5.1.</FONT></H2>
</TD>
<TD VALIGN=top>
<H2>Properties of Principal Components</H2>
</TD>
</TR >
</TABLE>
</TD>
</TR>






<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
<b><u>Orthogonalizing Transformations</u></b>

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
Transformation from \({\bf z}\) to \({\bf y}\)
<!-- <b><i>z</i></b> to <b><i>y</i></b> -->
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
The equation \({\bf y} = {\bf V}'{\bf z}\)
<!-- <b><i>y = V'z</i></b> -->
represents a transformation, where \({\bf y}\)
<!-- <b><i>y</i></b> -->
is the transformed variable, \({\bf z}\)
<!-- <b>z</b> -->
is the original standardized variable, and \({\bf V}\)
<!-- <b><i>V</i></b> -->
is the premultiplier to go from \({\bf z}\) to \({\bf y}\).
<!-- <b><i>z</i></b> to <b><i>y</i></b>. -->

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<i>Orthogonal transformations simplify things</i>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
To produce a transformation vector for \({\bf y}\)
<!-- <b>y</b> -->
for which the elements are uncorrelated is the same as 
saying that we want \({\bf V}\)
<!-- <b>V</b> -->
such that \({\bf D}_{\bf y}\)
<!-- <b>D<sub>y</sub></b> -->
is a diagonal matrix.  That 
is, all the off-diagonal elements of \({\bf D}_{\bf y}\)
<!-- <b>D<sub>y</sub></b> -->
must be zero.  This is called an <i>orthogonalizing transformation</i>.

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
Infinite number of values for \({\bf V}\)
<!-- <b><i>V</i></b> -->
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
There are an infinite number of values for \({\bf V}\)
<!-- <b>V</b> -->
that will produce a diagonal \({\bf D}_{\bf y}\)
<!-- <b>D<sub>y</sub></b> -->
for any correlation matrix \({\bf R}\).
<!-- <b>R</b>. -->
Thus the mathematical problem "find a unique \({\bf V}\)
<!-- <b>V</b> -->
such that \({\bf D}_{\bf y}\)
<!-- <b>D<sub>y</sub></b> -->
is diagonal" cannot be solved as it stands. A
number of famous statisticians such as Karl Pearson and Harold
Hotelling pondered this problem and suggested a "variance maximizing"
solution.

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<i>Principal components maximize variance of the
transformed elements, one by one</i>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
Hotelling (1933) derived the "principal components" solution.  It
proceeds as follows: for the first principal component, which will
be the first element of \({\bf y}\)
<!-- <b>y</b> -->
and be defined by the coefficients
in the first column of \({\bf V}\),
<!-- <b>V</b>, -->
(denoted by \({\bf v}_1\)),
<!-- <b>v</b><sub>1</sub>), -->
we want a solution such that the variance of \({\bf y}_1\)
<!-- <b>y</b><sub>1</sub> -->
will be maximized.

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
Constrain \({\bf v}\)
<!-- <b><i>v</i></b>  -->
to generate a unique solution
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
The constraint on the numbers in \({\bf v}_1\)
<!-- <b>v</b><sub>1</sub> -->
is that the
sum of the squares of the coefficients equals 1.  Expressed
mathematically, we wish to maximize

$$ \frac{1}{N} \sum_{i=1}^N Y_{1i}^2 \, , $$

<!-- 
<center>
<img SRC="eqns/princo1.gif" height=89 width=58
     ALT="(1/N)*SUM[i=1 to N]y(1i)**2">
</center>
-->

where

$$ y_{1i} = {\bf v}_1' {\bf z}_i \, , $$
<!-- 
<center>
<i>y</i><sub>1<i>i</i></sub> = <b>v<sub>1</sub></b>'<b><sub>&nbsp;
</sub>z<sub>i</sub></b>
</center>
<p>
-->
and \({\bf v}_1'{\bf v}_1 = 1\)
<!-- <b>v<sub>1</sub>'v<sub>1</sub></b> = 1 -->
(this is called "normalizing" \({\bf v}_1\)).
<!-- <b>v</b><sub>1</sub>). -->

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
Computation of first principal component from 
\({\bf R}\) and \({\bf v}_1\)
<!-- <b><i>R</i></b> and <b><i>v</i><sub>1</sub></b> -->
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
Substituting the middle equation in the first yields

$$ \frac{1}{N} \sum_{i=1}^N Y_{1i}^2 = {\bf v}_1' {\bf R} {\bf v}_1 \, , $$

<!--
<center>
<img SRC="eqns/princo2.gif" height=89 width=120 ALT=
"(1/N)*SUM[i=1 to N]y(1i)**2 = v(1)'*R*v(1)">
</center>
-->

where \({\bf R}\)
<!-- <b>R</b> -->
is the correlation matrix of \({\bf Z}\),
<!-- <b>Z</b>, -->
which, in turn, is the standardized matrix of \({\bf X}\),
<!-- <b>X</b>, -->
the original data matrix.  Therefore, we want to maximize
\({\bf v}_1' {\bf R} {\bf v}_1\)
<!-- <b>v<sub>1</sub>'Rv<sub>1</sub></b> -->
subject to \({\bf v}_1'{\bf v}_1 = 1\).
<!-- <b>v<sub>1</sub>'v<sub>1</sub></b> = 1. -->

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
<b><u>The eigenstructure</u></b>

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
Lagrange multiplier approach
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
Let

$$ \phi_1 = {\bf v}_1' {\bf R} {\bf v}_1 - \lambda_1({\bf v}_1'{\bf v}_1 - 1) $$

<!--
<center>
<img SRC="eqns/princo3.gif" height=70 width=173
     ALT="phi(1) = v(1)'*R*v(1) - lambda(1)*(v(1)'*v(1) - 1)">>
</center>
-->

introduce the restriction on \({\bf v}_1\)
<!-- <b>v</b><sub>1</sub> -->
via the Lagrange multiplier approach.  It can be shown
(<a href="../section7/pmc7.html#T.W.">T.W. Anderson,
1958, page 347</a>, theorem 8) that the vector of partial derivatives is

$$ \frac{\partial \phi_1}{\partial {\bf v}_1} = 2 {\bf R} {\bf v}_1 - 2 \lambda_1 {\bf v}_1 \, , $$

<!--
<center>
<img SRC="eqns/princo4.gif" height=90 width=129 alt=
"Partial derivative phi(1) with respect to v(1) =
 2*R*v(1) - 2*lambda(1)*v(1)">
</center>
-->

and setting this equal to zero, dividing out 2, and factoring, gives

$$ ({\bf R} - \lambda_1 {\bf I}) {\bf v}_1 = 0 \, . $$

<!-- 
<center>
<img SRC="eqns/princo5.gif" height=70 width=100
     ALT="(R - lambda(1)*I)*v(1) = 0">
</center>
-->

This is known as "the problem of the eigenstructure of \({\bf R}\)".
<!-- <b>R</b>". -->

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
Set of \(p\)
<!-- <i>p</i> -->
homogeneous equations
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
The partial differentiation resulted in a set of \(p\)
<!-- <i>p</i> -->
homogeneous equations, which may be written in matrix form as follows.


$$ 
\left[ \begin{array}{cccc} 
(1-\lambda_i) & r_{12} & \cdots & r_{1p} \\
r_{21} & (1-\lambda_i) & \cdots & r_{2p} \\
\vdots & \vdots &      & \vdots \\
r_{p1} & r_{p2} & \cdots & (1-\lambda_i) 
\end{array} \right]
\left[ \begin{array}{c} 
v_{1i} \\ v_{2i} \\ \vdots \\ v_{pi} 
\end{array} \right]
=
\left[ \begin{array}{c} 
0 \\ 0 \\ \vdots \\ 0 
\end{array} \right] 
$$

<!--
<center>
<img SRC="eqns/princo6.gif" height=122 width=282 ALT=
"[(1-lambda(i))  r(12)  ...  r(1p);
 r(21)  (1 - lambda(i))  ...  r(2p);
        ....                       ;
 r(p1)  r(p2)  ...  (1 - lambda(i))]*[v(1i)  v(2i)  ... v(pi)] =
 [0  0  ...  0  0]">
</center>
-->
</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
<b><u>The characteristic equation</u></b>

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
Characterstic equation of \({\bf R}\) 
<!-- <b><i>R</i></b> -->
is a polynomial of degree \(p\)
<!-- <i>p</i> -->
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
The characteristic equation of \({\bf R}\) 
<!-- <b>R</b> -->
is a polynomial of degree \(p\),
<!-- <i>p</i>, -->
which is obtained by expanding the determinant of

$$ |{\bf R} - \lambda {\bf I}| =

\left| \begin{array}{cccc} 
r_{11}-\lambda & r_{12} & \cdots & r_{1p} \\
r_{21} & r_{22}-\lambda  & \cdots & r_{2p} \\
\vdots & \vdots &      & \vdots \\
r_{p1} & r_{p2} & \cdots & r_{pp}-\lambda 
\end{array} \right|

= 0 \, , $$

<!--
<center>
<img SRC="eqns/princo7.gif" height=168 width=272 ALT="|R - lambda*I| =
 |[r(11)-lambda  ...  r(1p); r(21)  r(22)-lambda  ...  r(2p);
             ....         ;
 r(p1)  r(p2)  ...  r(pp)-lambda]| = 0">
</center>
-->
and solving for the roots \(\lambda_j, \, j = 1, \, 2, \, \ldots, \, p\).
<!-- <img align=absmiddle src="eqns/lambda.gif" alt="lambda">
<i><sub>j</sub></i>,&nbsp;<i>j</i> = 1, 2, ..., <i>p</i>. -->

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
Largest eigenvalue
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
Specifically, the largest eigenvalue, \(\lambda_1\),
<!-- <img align=absmiddle src="eqns/lambda.gif" alt="lambda"><sub>1</sub>, -->
and its associated vector, \({\bf v}_1\),
<!-- <b>v<sub>1</sub></b>, -->
are required.
Solving for this eigenvalue and vector is another mammoth numerical
task that can realistically only be performed by a computer.  In
general, software is involved and the algorithms are complex.

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
Remaining \(p\)
<!-- <i>p</i> -->
eigenvalues
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
After obtaining the first eigenvalue, the process is repeated until
all \(p\)
<!-- <i>p</i> -->
eigenvalues are computed.

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
Full eigenstructure of \({\bf R}\)
<!-- <b>R</b> -->
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
To succinctly define the full eigenstructure of \({\bf R}\),
<!-- <b>R</b>, -->
we introduce another matrix \({\bf L}\)
<!-- <b>L</b>, -->
which is a diagonal matrix with \(\lambda_j\)
<!-- <img align=absmiddle src="eqns/lambda.gif" alt="lambda"><sub><i>j</i></sub> -->
in the \(j\)th
<!-- <i>j</i>th -->
position on the diagonal.
Then the full eigenstructure of \({\bf R}\)
<!-- <b>R</b> -->
is given as
<ul> \({\bf RV} = {\bf VL}\),
   <!-- <b>RV = VL</b> -->
</ul>
where
<ul> \({\bf V}'{\bf V} = {\bf VV}' = {\bf I} \)
   <!-- <b>V'V = VV' = I</b> -->
</ul>
and
<ul> \({\bf V}'{\bf RV} = {\bf L} = {\bf D_y}\).
   <!-- <b>V'RV = L = D</b><sub> y</sub> -->
</ul>

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
<b><u>Principal Factors</u></b>

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
Scale to zero means and unit variances
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
It was mentioned before that it is helpful to scale any transformation \({\bf y}\)
<!-- <b>y</b> -->
of a vector variable \({\bf z}\)
<!-- <b>z</b> -->
so that its elements have zero
means and unit variances.  Such a standardized transformation is called
a <i>factoring</i> of \({\bf z}\), or of \({\bf R}\),
<!-- <b>z</b>, or of <b>R</b>, -->
and each linear component of the transformation is called a factor.

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
Deriving unit variances for principal components
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
Now, the principal components already have zero means, but their
variances are not 1; in fact, they are the eigenvalues,
comprising the diagonal elements of \({\bf L}\).
<!-- <b>L</b>. -->
It is possible to derive
the principal factor with unit variance from the principal component as
follows:

$$f_i = \frac{y_i}{\sqrt{\lambda}} \, , $$

<!--
<ul>
<img SRC="eqns/pc1.gif" ALT="f(i) = y(i)/SQRT(lambda(i))">
</ul>\-->

or for all factors,

$$ f = {\bf L}^{-1/2}{\bf y} \, . $$

<!--
<ul>
<img SRC="eqns/pc2.gif" ALT="f = L**(-1/2)*y"> 
</ul>
-->

Substituting \({\bf V}'{\bf z}\) for \({\bf y}\)
<!-- <b><i>V'z</i></b> for <b><i>y</i></b> -->
we have

$$f = {\bf L}^{-1/2} {\bf V}' {\bf z} = {\bf B}'{\bf z} \, , $$

<!--
<ul>
<img SRC="eqns/pc3.gif" ALT="f = L**(-1/2)*V'*z = B'*z">
</ul>
-->
<!-- <img SRC="eqns/princo8.gif" height=145 width=261> -->

where

$${\bf B} = {\bf VL}^{-1/2} \, . $$

<!--
<ul>
<b>B = VL</b><sup> -1/2</sup>
</ul>
-->
</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
\({\bf B}\) matrix
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
The matrix \({\bf B}\)
<!-- <b>B</b> -->
is then the matrix of <i>factor score coefficients</i> 
for principal factors.

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
<b><u>How many Eigenvalues?</u></b>

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
Dimensionality of the set of factor scores
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
The number of eigenvalues, \(N\),
<!-- <i>N</i>, -->
used in the final set determines
the dimensionality of the set of factor scores.  For example, if the
original test consisted of 8 measurements on 100 subjects, and we
extract 2 eigenvalues, the set of factor scores is a matrix of 100
rows by 2 columns.

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
Eigenvalues greater than unity
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
Each column or principal factor should represent a number of original
variables.  Kaiser (1966) suggested a rule-of-thumb that takes as
a value for \(N\),
<!-- <i>N</i>, -->
<u>the number of eigenvalues larger than unity</u>.

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
<b><u>Factor Structure</u></b>

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
Factor structure matrix \({\bf S}\)
<!-- <b>S</b> -->
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
The primary interpretative device in principal components is the factor
structure, computed as

$$ {\bf S} = {\bf VL}^{1/2} \, . $$ 

<!--
<ul>
<b>S = VL</b><sup>1/2</sup>
</ul>
-->

\({\bf S}\)
<!-- <b>S</b> -->
is a matrix whose elements are the correlations between the
principal components and the variables.  If we retain, for example, two
eigenvalues, meaning that there are two principal components, then the \({\bf S}\)
<!-- <b>S</b> -->
matrix consists of two columns and \(p\)
<!-- <i>p</i> -->
(number of variables) rows.

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
Table showing relation between variables and principal components
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
<center>
<table width=250>
<tr>
   <th valign=top align=center>
      &nbsp;
   </th>
   <th colspan=2 valign=top align=center>
      Principal Component
   </th>
</tr>
<tr>
   <th valign=top align=center>
      Variable
   </th>
   <th valign=top align=center>
      1
   </th>
   <th valign=top align=center>
      2
   </th>
</tr>
<tr>
   <td colspan=3>
      <hr noshade>
   </td>
</tr>
<tr>
   <th valign=top align=center>
      1
   </th>
   <td valign=top align=center>
      \(r_{11}\)
      <!-- <i>r</i><sub>11</sub> -->
   </td>
   <td valign=top align=center>
      \(r_{12}\)
      <!-- <i>r</i><sub>12</sub> -->
   </td>
</tr>
<tr>
   <th valign=top align=center>
      2
   </th>
   <td valign=top align=center>
      \(r_{21}\)
      <!-- <i>r</i><sub>21</sub> -->
   </td>
   <td valign=top align=center>
      \(r_{22}\)
      <!-- <i>r</i><sub>22</sub> -->
   </td>
</tr>
<tr>
   <th valign=top align=center>
      3
   </th>
   <td valign=top align=center>
      \(r_{31}\)
      <!-- <i>r</i><sub>31</sub> -->
   </td>
   <td valign=top align=center>
      \(r_{32}\)
      <!-- <i>r</i><sub>32</sub> -->
   </td>
</tr>
<tr>
   <th valign=top align=center>
      4
   </th>
   <td valign=top align=center>
      \(r_{41}\)
      <!-- <i>r</i><sub>41</sub> -->
   </td>
   <td valign=top align=center>
      \(r_{42}\)
      <!-- <i>r</i><sub>42</sub> -->
   </td>
</tr>
</table>
</center>
<p>
The \(r_{ij}\)
<!-- r<i><sub>ij</sub></i> -->
are the correlation coefficients between variable \(i\)
<!-- <i>i</i> -->
and principal component \(j\), where \(i\)
<!-- <i>j</i>, where <i>i</i> -->
ranges from 1 to 4 and \(j\)
<!-- <i>j</i> -->
from 1 to 2.
<!-- <img SRC="eqns/princo9.gif" height=224 width=221> -->

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
The communality
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
\({\bf SS}'\)
<!-- <b>SS</b>' -->
is the source of the "explained" correlations among
the variables. Its diagonal is called 
<i><a href="#measure">"the communality"</a></i>.

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
<b><u>Rotation</u></b>

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
Factor analysis
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
If this correlation matrix, i.e., the factor structure matrix,
does not help much in the interpretation, it is possible to rotate the
axis of the principal components.  This may result in the polarization
of the correlation coefficients.  Some practitioners refer
to rotation after generating the factor structure as 
<i>factor analysis</i>.

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
Varimax rotation
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
A popular scheme for rotation was suggested by Henry Kaiser in 1958.
He produced a method for orthogonal rotation of factors, called the
varimax rotation, which cleans up the factors as follows:
<ul>
   <i>For each factor, high loadings (correlations) will result
   for a few variables; the rest will be near zero.</i>
</ul>

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
Example
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
The following computer output from a principal component analysis on
a four-variable data set, followed by varimax rotation of the factor
structure, will illustrate his point.
<p>
<center>
<table width=300>
<tr>
   <th align=center valign=top>
      &nbsp;
   </th>
   <th colspan=2 align=center valign=top>
      Before Rotation
   </th>
   <th colspan=2 align=center valign=top>
      After Rotation
   </th>
</tr>
<tr>
   <th align=center valign=top>
      Variable
   </th>
   <th align=center valign=top>
      Factor 1
   </th>
   <th align=center valign=top>
      Factor 2
   </th>
   <th align=center valign=top>
      Factor 1
   </th>
   <th align=center valign=top>
      Factor 2
   </th>
</tr>
<tr>
   <td colspan=5>
      <hr noshade>
   </td>
</tr>
<tr>
   <th align=right valign=top>
      1
   </th>
   <td align=right valign=top>
      0.853
   </td>
   <td align=right valign=top>
      -0.989
   </td>
   <td align=right valign=top>
      0.997
   </td>
   <td align=right valign=top>
      0.058
   </td>
</tr>
<tr>
   <th align=right valign=top>
      2
   </th>
   <td align=right valign=top>
      0.634
   </td>
   <td align=right valign=top>
      0.762
   </td>
   <td align=right valign=top>
      0.089
   </td>
   <td align=right valign=top>
      0.987
   </td>
</tr>
<tr>
   <th align=right valign=top>
      3
   </th>
   <td align=right valign=top>
      0.858
   </td>
   <td align=right valign=top>
      -0.498
   </td>
   <td align=right valign=top>
      0.989
   </td>
   <td align=right valign=top>
      0.076
   </td>
</tr>
<tr>
   <th align=right valign=top>
      4
   </th>
   <td align=right valign=top>
      0.633
   </td>
   <td align=right valign=top>
      0.736
   </td>
   <td align=right valign=top>
      0.103
   </td>
   <td align=right valign=top>
      0.965
   </td>
</tr>
</table>
</center>
<!-- <img SRC="eqns/princ11.gif" height=188 width=309> -->

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
<b><u>Communality</u></b>

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
Formula for communality statistic
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
<a NAME="measure"></a>A measure of how well the selected factors
(principal components) "explain" the variance of each of the variables
is given by a statistic called <i>communality</i>.  This is defined by

$$ h_k^2 = \sum_{i=1}^k S_{ki}^2 \, . $$

<!--
<center>
<img SRC="eqns/princ10.gif" height=90 width=72
     ALT="h(k)**2 = SUM[i=1 to k]S(ki)**2">
</center>
-->
</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
Explanation of communality statistic
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
That is: the square of the correlation of variable \(k\)
<!-- <i>k</i> -->
with factor \(i\)
<!-- <i>i</i> -->
gives the part of the variance accounted for by that
factor.  The sum of these squares for \(n\)
<!-- <i>n</i> -->
factors is the
communality, or explained variable for that variable (row).

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
<b><u>Roadmap to solve the V matrix </u></b>

</TD>
</TR>
<!-- end paragraph -->

<!-- begin paragraph -->
<TR>
<TD WIDTH=15% VALIGN=top>
<!-- Add marginal notes below -->
<I>
Main steps to obtaining eigenstructure for a correlation matrix
</I>
</TD>
<TD VALIGN=TOP WIDTH=85%>
<!-- Add main text below -->
In summary, here are the main steps to obtain the eigenstructure for
a correlation matrix.
<ol>
   <li>Compute \({\bf R}\),
       <!-- <b>R</b>, -->
       the correlation matrix of the original
       data.  \({\bf R}\)
       <!-- <b>R</b> -->
       is also the correlation matrix of the
       standardized data.
       <p>
   <li>Obtain the characteristic equation of \({\bf R}\)
       <!-- <b>R</b> -->
       which is a
       polynomial of degree \(p\)
       <!-- <i>p</i> -->
       (the number of variables),
       obtained from expanding the determinant of \(|{\bf R} - \lambda{\bf I}|=0\)
       <!-- |<b>R</b>-<img align=absmiddle src="eqns/lambda.gif" alt="lambda">
       <b>I</b>| = 0 -->
       and solving for the roots \(\lambda_p\),
       <!-- <img align=absmiddle src="eqns/lambda.gif" alt="lambda">
       <i><sub>i</sub></i>, -->
       that is:  \(\lambda_1, \, \lambda_2, \, \ldots, \, \lambda_p\).
       <!-- <img align=absmiddle src="eqns/lambda.gif" alt="lambda">
       <sub>1</sub>,
       <img align=absmiddle src="eqns/lambda.gif" alt="lambda">
       <sub>2</sub>, ... ,
       <img align=absmiddle src="eqns/lambda.gif" alt="lambda">
       <sub>p</sub>. -->
       <p>
   <li>Then solve for the columns of the \({\bf V}\)
       <!-- <b>V</b> -->
       matrix, (\({\bf v}_1, \, {\bf v}_2\, \, \ldots, \, {\bf v}_p\)). 
       <!-- (<b>v<sub>1</sub>, v<sub>2</sub>, ..v<sub>p</sub></b>). -->
       The roots, \(\lambda_i\),
       <!-- <img align=absmiddle src="eqns/lambda.gif" alt="lambda">,<i><sub>i</sub></i>, -->
       are called the <i>eigenvalue</i>s (or
       latent values).  The columns of \({\bf V}\)
       <!-- <b>V</b> -->
       are called the <i>eigenvectors</i>.
</ol>

</TD>
</TR>
<!-- end paragraph -->







   
</TABLE>

<IMG SRC="../../gifs/nvgbrbtm.gif" BORDER=0 USEMAP="#nvbar.nvbar">
<map name="nvbar.nvbar">
<area shape="rect" href="http://www.nist.gov/" coords="22,6,67,20">
<area shape="rect" href="http://www.sematech.org/" coords="3,23,92,40">
<area shape="rect" alt="Home" href="../../index-2.html" coords="114,12,165,31">
<area shape="rect" alt="Tools & Aids" href="../../toolaids.html" coords="190,12,290,31">
<area shape="rect" alt="Search Handbook" href="../../search.html" coords="318,14,376,30">
<area shape="rect" alt="Previous Page" href="pmc55.html" coords="428,15,471,29">
<area shape="rect" alt="Next Page" href="pmc552.html" coords="476,15,517,30">
<area shape="default" nohref>
</map>
   
</BODY>


<!-- Mirrored from www.itl.nist.gov/div898/handbook/pmc/section5/pmc551.htm by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 17 Feb 2017 22:22:06 GMT -->
</HTML>
